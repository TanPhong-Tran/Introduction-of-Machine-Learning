{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thư viện\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Species  Weight  Length1  Length2  Length3   Height   Width\n",
      "0   Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
      "1   Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
      "2   Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
      "3   Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
      "4   Bream   430.0     26.5     29.0     34.0  12.4440  5.1340\n"
     ]
    }
   ],
   "source": [
    "#đọc file bằng pandas\n",
    "df=pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-02/Fish.csv\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "X_0 = df['Height'].values.reshape(-1,1)\n",
    "y = df['Weight'].values  \n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27514f314e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Vẽ thử\n",
    "sns.scatterplot(x=\"Height\",y=\"Weight\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 1)\n",
      "(127, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df.loc[:,[\"Height\"]].values,df.loc[:,\"Weight\"].values,test_size=0.2,random_state=1)\n",
    "y_train=y_train.reshape(-1,1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Câu 1: Dùng 2 thuật toán Gradient Descent còn lại để tìm bộ Weight theo Heigh trong data fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape của X là: (127, 2)\n",
      "\n",
      "Loss at iter 0: 3477208.940843345\n",
      "Loss at iter 1000: 68965.64825510944\n",
      "Loss at iter 2000: 68710.08434006629\n",
      "Loss at iter 3000: 68463.40416526844\n",
      "Loss at iter 4000: 68225.29421958774\n",
      "Loss at iter 5000: 67995.45675381439\n",
      "Loss at iter 6000: 67773.60436322188\n",
      "Loss at iter 7000: 67559.45962817616\n",
      "Loss at iter 8000: 67352.7547672309\n",
      "Loss at iter 9000: 67153.23130227489\n",
      "Loss at iter 10000: 66960.63973531325\n",
      "Loss at iter 11000: 66774.73923647754\n",
      "Loss at iter 12000: 66595.29734287558\n",
      "Loss at iter 13000: 66422.08966790365\n",
      "Loss at iter 14000: 66254.8996206579\n",
      "Loss at iter 15000: 66093.51813509398\n",
      "Loss at iter 16000: 65937.74340859649\n",
      "Loss at iter 17000: 65787.3806496308\n",
      "Loss at iter 18000: 65642.24183416228\n",
      "Loss at iter 19000: 65502.14547053806\n",
      "Loss at iter 20000: 65366.9163725371\n",
      "Loss at iter 21000: 65236.38544030536\n",
      "Loss at iter 22000: 65110.389448901435\n",
      "Loss at iter 23000: 64988.77084418879\n",
      "Loss at iter 24000: 64871.37754581931\n",
      "Loss at iter 25000: 64758.062757061154\n",
      "Loss at iter 26000: 64648.68478123424\n",
      "Loss at iter 27000: 64543.10684452265\n",
      "Loss at iter 28000: 64441.19692494325\n",
      "Loss at iter 29000: 64342.82758725626\n",
      "Loss at iter 30000: 64247.875823611335\n",
      "Loss at iter 31000: 64156.22289973001\n",
      "Loss at iter 32000: 64067.75420643213\n",
      "Loss at iter 33000: 63982.35911632055\n",
      "Loss at iter 34000: 63899.930845444986\n",
      "Loss at iter 35000: 63820.36631977195\n",
      "Loss at iter 36000: 63743.56604629381\n",
      "Loss at iter 37000: 63669.433988615725\n",
      "Loss at iter 38000: 63597.877446865314\n",
      "Loss at iter 39000: 63528.80694177404\n",
      "Loss at iter 40000: 63462.13610278627\n",
      "Loss at iter 41000: 63397.78156005549\n",
      "Loss at iter 42000: 63335.66284019287\n",
      "Loss at iter 43000: 63275.70226563771\n",
      "Loss at iter 44000: 63217.8248575241\n",
      "Loss at iter 45000: 63161.95824192208\n",
      "Loss at iter 46000: 63108.03255933628\n",
      "Loss at iter 47000: 63055.9803773487\n",
      "Loss at iter 48000: 63005.736606296676\n",
      "Loss at iter 49000: 62957.23841788004\n",
      "Loss at iter 50000: 62910.425166596426\n",
      "Loss at iter 51000: 62865.23831390583\n",
      "Loss at iter 52000: 62821.62135503001\n",
      "Loss at iter 53000: 62779.51974829494\n",
      "Loss at iter 54000: 62738.880846928114\n",
      "Loss at iter 55000: 62699.65383322542\n",
      "Loss at iter 56000: 62661.78965500512\n",
      "Loss at iter 57000: 62625.240964269564\n",
      "Loss at iter 58000: 62589.962057998004\n",
      "Loss at iter 59000: 62555.90882099636\n",
      "Loss at iter 60000: 62523.03867073255\n",
      "Loss at iter 61000: 62491.31050408821\n",
      "Loss at iter 62000: 62460.68464596071\n",
      "Loss at iter 63000: 62431.12279965046\n",
      "Loss at iter 64000: 62402.58799897209\n",
      "Loss at iter 65000: 62375.04456202948\n",
      "Loss at iter 66000: 62348.458046596556\n",
      "Loss at iter 67000: 62322.795207048286\n",
      "Loss at iter 68000: 62298.02395278816\n",
      "Loss at iter 69000: 62274.1133081198\n",
      "Loss at iter 70000: 62251.0333735128\n",
      "Loss at iter 71000: 62228.75528821435\n",
      "Loss at iter 72000: 62207.25119415963\n",
      "Loss at iter 73000: 62186.49420113628\n",
      "Loss at iter 74000: 62166.45835315888\n",
      "Loss at iter 75000: 62147.11859601192\n",
      "Loss at iter 76000: 62128.45074592013\n",
      "Loss at iter 77000: 62110.43145930752\n",
      "Loss at iter 78000: 62093.038203606884\n",
      "Loss at iter 79000: 62076.24922908357\n",
      "Loss at iter 80000: 62060.04354163804\n",
      "Loss at iter 81000: 62044.40087655346\n",
      "Loss at iter 82000: 62029.30167315518\n",
      "Loss at iter 83000: 62014.727050350855\n",
      "Loss at iter 84000: 62000.658783020095\n",
      "Loss at iter 85000: 61987.07927922456\n",
      "Loss at iter 86000: 61973.97155820981\n",
      "Loss at iter 87000: 61961.31922917127\n",
      "Loss at iter 88000: 61949.10647075821\n",
      "Loss at iter 89000: 61937.31801128948\n",
      "Loss at iter 90000: 61925.93910965678\n",
      "Loss at iter 91000: 61914.95553689141\n",
      "Loss at iter 92000: 61904.353558371295\n",
      "Loss at iter 93000: 61894.11991664635\n",
      "Loss at iter 94000: 61884.24181486033\n",
      "Loss at iter 95000: 61874.70690074888\n",
      "Loss at iter 96000: 61865.503251193295\n",
      "Loss at iter 97000: 61856.619357311116\n",
      "Loss at iter 98000: 61848.04411006455\n",
      "Loss at iter 99000: 61839.766786368964\n",
      "Loss at iter 100000: 61831.77703568411\n",
      "Loss at iter 101000: 61824.06486707091\n",
      "Loss at iter 102000: 61816.62063669822\n",
      "Loss at iter 103000: 61809.43503578348\n",
      "Loss at iter 104000: 61802.499078952365\n",
      "Loss at iter 105000: 61795.804093002815\n",
      "Loss at iter 106000: 61789.34170605957\n",
      "Loss at iter 107000: 61783.10383710519\n",
      "Loss at iter 108000: 61777.08268587523\n",
      "Loss at iter 109000: 61771.270723104106\n",
      "Loss at iter 110000: 61765.66068111\n",
      "Loss at iter 111000: 61760.24554470685\n",
      "Loss at iter 112000: 61755.01854243208\n",
      "Loss at iter 113000: 61749.97313807911\n",
      "Loss at iter 114000: 61745.10302252397\n",
      "Loss at iter 115000: 61740.40210583602\n",
      "Loss at iter 116000: 61735.86450966264\n",
      "Loss at iter 117000: 61731.48455987847\n",
      "Loss at iter 118000: 61727.25677949016\n",
      "Loss at iter 119000: 61723.17588178757\n",
      "Loss at iter 120000: 61719.23676373281\n",
      "Loss at iter 121000: 61715.43449957918\n",
      "Loss at iter 122000: 61711.76433471156\n",
      "Loss at iter 123000: 61708.221679700924\n",
      "Loss at iter 124000: 61704.80210456546\n",
      "Loss at iter 125000: 61701.50133323099\n",
      "Loss at iter 126000: 61698.315238183844\n",
      "Loss at iter 127000: 61695.23983530953\n",
      "Loss at iter 128000: 61692.27127891073\n",
      "Loss at iter 129000: 61689.40585689842\n",
      "Loss at iter 130000: 61686.63998614996\n",
      "Loss at iter 131000: 61683.97020802846\n",
      "Loss at iter 132000: 61681.393184058026\n",
      "Loss at iter 133000: 61678.905691748914\n",
      "Loss at iter 134000: 61676.50462056802\n",
      "Loss at iter 135000: 61674.186968049224\n",
      "Loss at iter 136000: 61671.94983603888\n",
      "Loss at iter 137000: 61669.79042707179\n",
      "Loss at iter 138000: 61667.70604087303\n",
      "Loss at iter 139000: 61665.69407098136\n",
      "Loss at iter 140000: 61663.75200148993\n",
      "Loss at iter 141000: 61661.87740390018\n",
      "Loss at iter 142000: 61660.067934085135\n",
      "Loss at iter 143000: 61658.32132935808\n",
      "Loss at iter 144000: 61656.6354056432\n",
      "Loss at iter 145000: 61655.008054744394\n",
      "Loss at iter 146000: 61653.43724170912\n",
      "Loss at iter 147000: 61651.92100228367\n",
      "Loss at iter 148000: 61650.45744045701\n",
      "Loss at iter 149000: 61649.04472608977\n",
      "Loss at iter 150000: 61647.681092625826\n",
      "Loss at iter 151000: 61646.36483488321\n",
      "Loss at iter 152000: 61645.094306921834\n",
      "Loss at iter 153000: 61643.86791998533\n",
      "Loss at iter 154000: 61642.68414051429\n",
      "Loss at iter 155000: 61641.541488228686\n",
      "Loss at iter 156000: 61640.43853427678\n",
      "Loss at iter 157000: 61639.37389944831\n",
      "Loss at iter 158000: 61638.346252449985\n",
      "Loss at iter 159000: 61637.35430824066\n",
      "Loss at iter 160000: 61636.39682642438\n",
      "Loss at iter 161000: 61635.47260969943\n",
      "Loss at iter 162000: 61634.58050236103\n",
      "Loss at iter 163000: 61633.71938885623\n",
      "Loss at iter 164000: 61632.88819238887\n",
      "Loss at iter 165000: 61632.08587357322\n",
      "Loss at iter 166000: 61631.31142913408\n",
      "Loss at iter 167000: 61630.5638906524\n",
      "Loss at iter 168000: 61629.842323354125\n",
      "Loss at iter 169000: 61629.14582494137\n",
      "Loss at iter 170000: 61628.47352446421\n",
      "Loss at iter 171000: 61627.824581231405\n",
      "Loss at iter 172000: 61627.19818375931\n",
      "Loss at iter 173000: 61626.59354875702\n",
      "Loss at iter 174000: 61626.00992014692\n",
      "Loss at iter 175000: 61625.44656811929\n",
      "Loss at iter 176000: 61624.902788219646\n",
      "Loss at iter 177000: 61624.37790046784\n",
      "Loss at iter 178000: 61623.87124850776\n",
      "Loss at iter 179000: 61623.38219878664\n",
      "Loss at iter 180000: 61622.910139762724\n",
      "Loss at iter 181000: 61622.45448114067\n",
      "Loss at iter 182000: 61622.01465313325\n",
      "Loss at iter 183000: 61621.590105749005\n",
      "Loss at iter 184000: 61621.1803081044\n",
      "Loss at iter 185000: 61620.78474776003\n",
      "Loss at iter 186000: 61620.40293007975\n",
      "Loss at iter 187000: 61620.03437761221\n",
      "Loss at iter 188000: 61619.67862949384\n",
      "Loss at iter 189000: 61619.33524087246\n",
      "Loss at iter 190000: 61619.00378235115\n",
      "Loss at iter 191000: 61618.6838394511\n",
      "Loss at iter 192000: 61618.3750120935\n",
      "Loss at iter 193000: 61618.07691409921\n",
      "Loss at iter 194000: 61617.78917270576\n",
      "Loss at iter 195000: 61617.511428101374\n",
      "Loss at iter 196000: 61617.24333297488\n",
      "Loss at iter 197000: 61616.98455208154\n",
      "Loss at iter 198000: 61616.734761823725\n",
      "Loss at iter 199000: 61616.493649846365\n",
      "Loss at iter 200000: 61616.260914646315\n",
      "Loss at iter 201000: 61616.036265195326\n",
      "Loss at iter 202000: 61615.81942057609\n",
      "Loss at iter 203000: 61615.61010963106\n",
      "Loss at iter 204000: 61615.40807062331\n",
      "Loss at iter 205000: 61615.2130509092\n",
      "Loss at iter 206000: 61615.02480662258\n",
      "Loss at iter 207000: 61614.84310236968\n",
      "Loss at iter 208000: 61614.667710934926\n",
      "Loss at iter 209000: 61614.49841299664\n",
      "Loss at iter 210000: 61614.33499685299\n",
      "Loss at iter 211000: 61614.177258157026\n",
      "Loss at iter 212000: 61614.02499966136\n",
      "Loss at iter 213000: 61613.87803097137\n",
      "Loss at iter 214000: 61613.73616830723\n",
      "Loss at iter 215000: 61613.59923427399\n",
      "Loss at iter 216000: 61613.467057639864\n",
      "Loss at iter 217000: 61613.33947312196\n",
      "Loss at iter 218000: 61613.21632117977\n",
      "Loss at iter 219000: 61613.097447815555\n",
      "Loss at iter 220000: 61612.982704381786\n",
      "Loss at iter 221000: 61612.87194739533\n",
      "Loss at iter 222000: 61612.76503835793\n",
      "Loss at iter 223000: 61612.66184358315\n",
      "Loss at iter 224000: 61612.562234029036\n",
      "Loss at iter 225000: 61612.46608513689\n",
      "Loss at iter 226000: 61612.37327667546\n",
      "Loss at iter 227000: 61612.28369259064\n",
      "Loss at iter 228000: 61612.1972208602\n",
      "Loss at iter 229000: 61612.11375335396\n",
      "Loss at iter 230000: 61612.03318569832\n",
      "Loss at iter 231000: 61611.95541714586\n",
      "Loss at iter 232000: 61611.880350449406\n",
      "Loss at iter 233000: 61611.80789174038\n",
      "Loss at iter 234000: 61611.73795041134\n",
      "Loss at iter 235000: 61611.67043900279\n",
      "Loss at iter 236000: 61611.60527309382\n",
      "Loss at iter 237000: 61611.54237119639\n",
      "Loss at iter 238000: 61611.481654653646\n",
      "Loss at iter 239000: 61611.42304754141\n",
      "Loss at iter 240000: 61611.36647657325\n",
      "Loss at iter 241000: 61611.311871008904\n",
      "Loss at iter 242000: 61611.25916256579\n",
      "Loss at iter 243000: 61611.20828533363\n",
      "Loss at iter 244000: 61611.15917569199\n",
      "Loss at iter 245000: 61611.111772230724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 246000: 61611.06601567331\n",
      "Loss at iter 247000: 61611.021848802535\n",
      "Loss at iter 248000: 61610.97921638909\n",
      "Loss at iter 249000: 61610.938065122435\n",
      "Loss at iter 250000: 61610.8983435442\n",
      "Loss at iter 251000: 61610.860001983725\n",
      "Loss at iter 252000: 61610.822992496076\n",
      "Loss at iter 253000: 61610.78726880203\n",
      "Loss at iter 254000: 61610.75278623018\n",
      "Loss at iter 255000: 61610.71950166115\n",
      "Loss at iter 256000: 61610.68737347357\n",
      "Loss at iter 257000: 61610.65636149216\n",
      "Loss at iter 258000: 61610.62642693734\n",
      "Loss at iter 259000: 61610.59753237691\n",
      "Loss at iter 260000: 61610.5696416791\n",
      "Loss at iter 261000: 61610.54271996741\n",
      "Loss at iter 262000: 61610.51673357708\n",
      "Loss at iter 263000: 61610.49165001295\n",
      "Loss at iter 264000: 61610.467437908745\n",
      "Loss at iter 265000: 61610.444066988\n",
      "Loss at iter 266000: 61610.42150802608\n",
      "Loss at iter 267000: 61610.39973281369\n",
      "Loss at iter 268000: 61610.37871412161\n",
      "Loss at iter 269000: 61610.35842566657\n",
      "Loss at iter 270000: 61610.33884207853\n",
      "Loss at iter 271000: 61610.319938868764\n",
      "Loss at iter 272000: 61610.301692399415\n",
      "Loss at iter 273000: 61610.284079853824\n",
      "Loss at iter 274000: 61610.267079208024\n",
      "Loss at iter 275000: 61610.25066920326\n",
      "Loss at iter 276000: 61610.2348293193\n",
      "Loss at iter 277000: 61610.219539748854\n",
      "Loss at iter 278000: 61610.2047813728\n",
      "Loss at iter 279000: 61610.1905357362\n",
      "Loss at iter 280000: 61610.17678502537\n",
      "Loss at iter 281000: 61610.16351204543\n",
      "Loss at iter 282000: 61610.15070019892\n",
      "Loss at iter 283000: 61610.138333465045\n",
      "Loss at iter 284000: 61610.12639637959\n",
      "Loss at iter 285000: 61610.11487401557\n",
      "Loss at iter 286000: 61610.10375196462\n",
      "Loss at iter 287000: 61610.09301631896\n",
      "Loss at iter 288000: 61610.08265365399\n",
      "Loss at iter 289000: 61610.07265101153\n",
      "Loss at iter 290000: 61610.06299588356\n",
      "Loss at iter 291000: 61610.053676196636\n",
      "Loss at iter 292000: 61610.044680296814\n",
      "Loss at iter 293000: 61610.03599693496\n",
      "Loss at iter 294000: 61610.0276152528\n",
      "Loss at iter 295000: 61610.01952476931\n",
      "Loss at iter 296000: 61610.011715367575\n",
      "Loss at iter 297000: 61610.00417728218\n",
      "Loss at iter 298000: 61609.99690108696\n",
      "Loss at iter 299000: 61609.9898776833\n",
      "Loss at iter 300000: 61609.98309828861\n",
      "Loss at iter 301000: 61609.97655442548\n",
      "Loss at iter 302000: 61609.970237910995\n",
      "Loss at iter 303000: 61609.96414084658\n",
      "Loss at iter 304000: 61609.958255608\n",
      "Loss at iter 305000: 61609.95257483599\n",
      "Loss at iter 306000: 61609.947091426875\n",
      "Loss at iter 307000: 61609.94179852385\n",
      "Loss at iter 308000: 61609.93668950827\n",
      "Loss at iter 309000: 61609.93175799148\n",
      "Loss at iter 310000: 61609.92699780681\n",
      "Loss at iter 311000: 61609.92240300173\n",
      "Loss at iter 312000: 61609.917967830625\n",
      "Loss at iter 313000: 61609.913686747415\n",
      "Loss at iter 314000: 61609.90955439876\n",
      "Loss at iter 315000: 61609.90556561728\n",
      "Loss at iter 316000: 61609.901715415115\n",
      "Loss at iter 317000: 61609.89799897771\n",
      "Loss at iter 318000: 61609.89441165777\n",
      "Loss at iter 319000: 61609.89094896947\n",
      "Loss at iter 320000: 61609.88760658281\n",
      "Loss at iter 321000: 61609.88438031825\n",
      "Loss at iter 322000: 61609.88126614146\n",
      "Loss at iter 323000: 61609.878260158235\n",
      "Loss at iter 324000: 61609.875358609694\n",
      "Loss at iter 325000: 61609.872557867515\n",
      "Loss at iter 326000: 61609.869854429504\n",
      "Loss at iter 327000: 61609.86724491504\n",
      "Loss at iter 328000: 61609.86472606105\n",
      "Loss at iter 329000: 61609.86229471774\n",
      "Loss at iter 330000: 61609.85994784482\n",
      "Loss at iter 331000: 61609.85768250761\n",
      "Loss at iter 332000: 61609.85549587335\n",
      "Loss at iter 333000: 61609.85338520772\n",
      "Loss at iter 334000: 61609.8513478714\n",
      "Loss at iter 335000: 61609.84938131679\n",
      "Loss at iter 336000: 61609.84748308473\n",
      "Loss at iter 337000: 61609.84565080156\n",
      "Loss at iter 338000: 61609.843882176094\n",
      "Loss at iter 339000: 61609.842174996644\n",
      "Loss at iter 340000: 61609.84052712851\n",
      "Loss at iter 341000: 61609.83893651105\n",
      "Loss at iter 342000: 61609.837401155244\n",
      "Loss at iter 343000: 61609.835919141195\n",
      "Loss at iter 344000: 61609.83448861568\n",
      "Loss at iter 345000: 61609.833107789855\n",
      "Loss at iter 346000: 61609.83177493707\n",
      "Loss at iter 347000: 61609.83048839058\n",
      "Loss at iter 348000: 61609.82924654167\n",
      "Loss at iter 349000: 61609.82804783739\n",
      "Loss at iter 350000: 61609.82689077883\n",
      "Loss at iter 351000: 61609.8257739191\n",
      "Loss at iter 352000: 61609.82469586162\n",
      "Loss at iter 353000: 61609.823655258304\n",
      "Loss at iter 354000: 61609.82265080792\n",
      "Loss at iter 355000: 61609.82168125439\n",
      "Loss at iter 356000: 61609.82074538537\n",
      "Loss at iter 357000: 61609.819842030556\n",
      "Loss at iter 358000: 61609.81897006035\n",
      "Loss at iter 359000: 61609.81812838436\n",
      "Loss at iter 360000: 61609.81731595012\n",
      "Loss at iter 361000: 61609.81653174166\n",
      "Loss at iter 362000: 61609.815774778406\n",
      "Loss at iter 363000: 61609.815044113755\n",
      "Loss at iter 364000: 61609.814338834054\n",
      "Loss at iter 365000: 61609.813658057385\n",
      "Loss at iter 366000: 61609.813000932445\n",
      "Loss at iter 367000: 61609.81236663752\n",
      "Loss at iter 368000: 61609.811754379414\n",
      "Loss at iter 369000: 61609.811163392566\n",
      "Loss at iter 370000: 61609.81059293793\n",
      "Loss at iter 371000: 61609.810042302204\n",
      "Loss at iter 372000: 61609.80951079677\n",
      "Loss at iter 373000: 61609.80899775706\n",
      "Loss at iter 374000: 61609.80850254154\n",
      "Loss at iter 375000: 61609.8080245309\n",
      "Loss at iter 376000: 61609.80756312744\n",
      "Loss at iter 377000: 61609.8071177542\n",
      "Loss at iter 378000: 61609.806687854216\n",
      "Loss at iter 379000: 61609.806272889946\n",
      "Loss at iter 380000: 61609.805872342484\n",
      "Loss at iter 381000: 61609.80548571093\n",
      "Loss at iter 382000: 61609.80511251186\n",
      "Loss at iter 383000: 61609.80475227854\n",
      "Loss at iter 384000: 61609.80440456057\n",
      "Loss at iter 385000: 61609.804068923106\n",
      "Loss at iter 386000: 61609.80374494643\n",
      "Loss at iter 387000: 61609.80343222546\n",
      "Loss at iter 388000: 61609.80313036911\n",
      "Loss at iter 389000: 61609.802838999945\n",
      "Loss at iter 390000: 61609.80255775361\n",
      "Loss at iter 391000: 61609.802286278406\n",
      "Loss at iter 392000: 61609.80202423487\n",
      "Loss at iter 393000: 61609.801771295315\n",
      "Loss at iter 394000: 61609.80152714345\n",
      "Loss at iter 395000: 61609.801291473996\n",
      "Loss at iter 396000: 61609.8010639922\n",
      "Loss at iter 397000: 61609.80084441367\n",
      "Loss at iter 398000: 61609.800632463775\n",
      "Loss at iter 399000: 61609.80042787752\n",
      "Loss at iter 400000: 61609.80023039905\n",
      "Loss at iter 401000: 61609.800039781425\n",
      "Loss at iter 402000: 61609.79985578629\n",
      "Loss at iter 403000: 61609.79967818355\n",
      "Loss at iter 404000: 61609.79950675114\n",
      "Loss at iter 405000: 61609.799341274695\n",
      "Loss at iter 406000: 61609.79918154729\n",
      "Loss at iter 407000: 61609.79902736913\n",
      "Loss at iter 408000: 61609.79887854751\n",
      "Loss at iter 409000: 61609.79873489626\n",
      "Loss at iter 410000: 61609.7985962358\n",
      "Loss at iter 411000: 61609.79846239273\n",
      "Loss at iter 412000: 61609.79833319965\n",
      "Loss at iter 413000: 61609.79820849504\n",
      "Loss at iter 414000: 61609.79808812294\n",
      "Loss at iter 415000: 61609.797971932865\n",
      "Loss at iter 416000: 61609.79785977947\n",
      "Loss at iter 417000: 61609.79775152255\n",
      "Loss at iter 418000: 61609.797647026724\n",
      "Loss at iter 419000: 61609.797546161324\n",
      "Loss at iter 420000: 61609.7974488002\n",
      "Loss at iter 421000: 61609.79735482165\n",
      "Loss at iter 422000: 61609.79726410809\n",
      "Loss at iter 423000: 61609.79717654615\n",
      "Loss at iter 424000: 61609.797092026296\n",
      "Loss at iter 425000: 61609.79701044288\n",
      "Loss at iter 426000: 61609.79693169383\n",
      "Loss at iter 427000: 61609.79685568072\n",
      "Loss at iter 428000: 61609.79678230848\n",
      "Loss at iter 429000: 61609.79671148533\n",
      "Loss at iter 430000: 61609.79664312276\n",
      "Loss at iter 431000: 61609.79657713525\n",
      "Loss at iter 432000: 61609.79651344029\n",
      "Loss at iter 433000: 61609.79645195825\n",
      "Loss at iter 434000: 61609.79639261224\n",
      "Loss at iter 435000: 61609.796335328036\n",
      "Loss at iter 436000: 61609.796280034025\n",
      "Loss at iter 437000: 61609.796226661034\n",
      "Loss at iter 438000: 61609.79617514237\n",
      "Loss at iter 439000: 61609.79612541357\n",
      "Loss at iter 440000: 61609.79607741245\n",
      "Loss at iter 441000: 61609.796031079\n",
      "Loss at iter 442000: 61609.7959863553\n",
      "Loss at iter 443000: 61609.795943185396\n",
      "Loss at iter 444000: 61609.795901515296\n",
      "Loss at iter 445000: 61609.79586129289\n",
      "Loss at iter 446000: 61609.79582246795\n",
      "Loss at iter 447000: 61609.79578499186\n",
      "Loss at iter 448000: 61609.79574881775\n",
      "Loss at iter 449000: 61609.79571390044\n",
      "Loss at iter 450000: 61609.79568019623\n",
      "Loss at iter 451000: 61609.79564766299\n",
      "Loss at iter 452000: 61609.79561626\n",
      "Loss at iter 453000: 61609.79558594805\n",
      "Loss at iter 454000: 61609.79555668919\n",
      "Loss at iter 455000: 61609.79552844686\n",
      "Loss at iter 456000: 61609.79550118571\n",
      "Loss at iter 457000: 61609.79547487171\n",
      "Loss at iter 458000: 61609.79544947189\n",
      "Loss at iter 459000: 61609.79542495452\n",
      "Loss at iter 460000: 61609.795401288946\n",
      "Loss at iter 461000: 61609.79537844557\n",
      "Loss at iter 462000: 61609.795356395814\n",
      "Loss at iter 463000: 61609.79533511212\n",
      "Loss at iter 464000: 61609.79531456787\n",
      "Loss at iter 465000: 61609.79529473738\n",
      "Loss at iter 466000: 61609.795275595854\n",
      "Loss at iter 467000: 61609.79525711934\n",
      "Loss at iter 468000: 61609.795239284744\n",
      "Loss at iter 469000: 61609.79522206973\n",
      "Loss at iter 470000: 61609.79520545286\n",
      "Loss at iter 471000: 61609.79518941326\n",
      "Loss at iter 472000: 61609.79517393092\n",
      "Loss at iter 473000: 61609.79515898649\n",
      "Loss at iter 474000: 61609.79514456124\n",
      "Loss at iter 475000: 61609.79513063717\n",
      "Loss at iter 476000: 61609.79511719684\n",
      "Loss at iter 477000: 61609.79510422345\n",
      "Loss at iter 478000: 61609.79509170081\n",
      "Loss at iter 479000: 61609.795079613235\n",
      "Loss at iter 480000: 61609.795067945604\n",
      "Loss at iter 481000: 61609.7950566833\n",
      "Loss at iter 482000: 61609.79504581231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 483000: 61609.79503531901\n",
      "Loss at iter 484000: 61609.79502519024\n",
      "Loss at iter 485000: 61609.795015413394\n",
      "Loss at iter 486000: 61609.795005976215\n",
      "Loss at iter 487000: 61609.79499686689\n",
      "Loss at iter 488000: 61609.79498807405\n",
      "Loss at iter 489000: 61609.79497958669\n",
      "Loss at iter 490000: 61609.79497139421\n",
      "Loss at iter 491000: 61609.79496348634\n",
      "Loss at iter 492000: 61609.79495585321\n",
      "Loss at iter 493000: 61609.79494848528\n",
      "Loss at iter 494000: 61609.794941373344\n",
      "Loss at iter 495000: 61609.794934508456\n",
      "Loss at iter 496000: 61609.79492788208\n",
      "Loss at iter 497000: 61609.79492148595\n",
      "Loss at iter 498000: 61609.794915312006\n",
      "Loss at iter 499000: 61609.79490935257\n",
      "Loss at iter 500000: 61609.79490360019\n",
      "Loss at iter 501000: 61609.79489804765\n",
      "Loss at iter 502000: 61609.79489268801\n",
      "Loss at iter 503000: 61609.79488751458\n",
      "Loss at iter 504000: 61609.79488252088\n",
      "Loss at iter 505000: 61609.79487770069\n",
      "Loss at iter 506000: 61609.79487304794\n",
      "Loss at iter 507000: 61609.794868556855\n",
      "Loss at iter 508000: 61609.7948642218\n",
      "Loss at iter 509000: 61609.79486003736\n",
      "Loss at iter 510000: 61609.794855998276\n",
      "Loss at iter 511000: 61609.79485209953\n",
      "Loss at iter 512000: 61609.794848336234\n",
      "Loss at iter 513000: 61609.79484470369\n",
      "Loss at iter 514000: 61609.79484119734\n",
      "Loss at iter 515000: 61609.79483781283\n",
      "Loss at iter 516000: 61609.79483454588\n",
      "Loss at iter 517000: 61609.794831392435\n",
      "Loss at iter 518000: 61609.79482834857\n",
      "Loss at iter 519000: 61609.794825410434\n",
      "Loss at iter 520000: 61609.794822574375\n",
      "Loss at iter 521000: 61609.79481983685\n",
      "Loss at iter 522000: 61609.794817194444\n",
      "Loss at iter 523000: 61609.794814643814\n",
      "Loss at iter 524000: 61609.794812181826\n",
      "Loss at iter 525000: 61609.794809805375\n",
      "Loss at iter 526000: 61609.79480751146\n",
      "Loss at iter 527000: 61609.794805297286\n",
      "Loss at iter 528000: 61609.79480316\n",
      "Loss at iter 529000: 61609.794801096985\n",
      "Loss at iter 530000: 61609.79479910561\n",
      "Loss at iter 531000: 61609.79479718345\n",
      "Loss at iter 532000: 61609.794795328075\n",
      "Loss at iter 533000: 61609.79479353715\n",
      "Loss at iter 534000: 61609.794791808454\n",
      "Loss at iter 535000: 61609.79479013981\n",
      "Loss at iter 536000: 61609.794788529136\n",
      "Loss at iter 537000: 61609.794786974424\n",
      "Loss at iter 538000: 61609.79478547371\n",
      "Loss at iter 539000: 61609.79478402517\n",
      "Loss at iter 540000: 61609.79478262692\n",
      "Loss at iter 541000: 61609.79478127726\n",
      "Loss at iter 542000: 61609.79477997451\n",
      "Loss at iter 543000: 61609.794778717005\n",
      "Loss at iter 544000: 61609.794777503186\n",
      "Loss at iter 545000: 61609.79477633154\n",
      "Loss at iter 546000: 61609.794775200586\n",
      "Loss at iter 547000: 61609.794774108945\n",
      "Loss at iter 548000: 61609.79477305523\n",
      "Loss at iter 549000: 61609.7947720381\n",
      "Loss at iter 550000: 61609.79477105632\n",
      "Loss at iter 551000: 61609.79477010866\n",
      "Loss at iter 552000: 61609.79476919392\n",
      "Loss at iter 553000: 61609.79476831096\n",
      "Loss at iter 554000: 61609.79476745866\n",
      "Loss at iter 555000: 61609.79476663598\n",
      "Loss at iter 556000: 61609.79476584189\n",
      "Loss at iter 557000: 61609.794765075385\n",
      "Loss at iter 558000: 61609.7947643355\n",
      "Loss at iter 559000: 61609.79476362132\n",
      "Loss at iter 560000: 61609.794762931975\n",
      "Loss at iter 561000: 61609.79476226655\n",
      "Loss at iter 562000: 61609.79476162427\n",
      "Loss at iter 563000: 61609.79476100428\n",
      "Loss at iter 564000: 61609.79476040585\n",
      "Loss at iter 565000: 61609.79475982821\n",
      "Loss at iter 566000: 61609.79475927063\n",
      "Loss at iter 567000: 61609.79475873243\n",
      "Loss at iter 568000: 61609.79475821292\n",
      "Loss at iter 569000: 61609.79475771147\n",
      "Loss at iter 570000: 61609.794757227435\n",
      "Loss at iter 571000: 61609.794756760195\n",
      "Loss at iter 572000: 61609.7947563092\n",
      "Loss at iter 573000: 61609.79475587389\n",
      "Loss at iter 574000: 61609.794755453695\n",
      "Loss at iter 575000: 61609.794755048104\n",
      "Loss at iter 576000: 61609.7947546566\n",
      "Loss at iter 577000: 61609.79475427869\n",
      "Loss at iter 578000: 61609.79475391391\n",
      "Loss at iter 579000: 61609.7947535618\n",
      "Loss at iter 580000: 61609.794753221955\n",
      "Loss at iter 581000: 61609.7947528939\n",
      "Loss at iter 582000: 61609.794752577225\n",
      "Loss at iter 583000: 61609.79475227156\n",
      "Loss at iter 584000: 61609.79475197651\n",
      "Loss at iter 585000: 61609.79475169172\n",
      "Loss at iter 586000: 61609.79475141683\n",
      "Loss at iter 587000: 61609.79475115149\n",
      "Loss at iter 588000: 61609.79475089535\n",
      "Loss at iter 589000: 61609.79475064812\n",
      "Loss at iter 590000: 61609.79475040948\n",
      "Loss at iter 591000: 61609.794750179135\n",
      "Loss at iter 592000: 61609.794749956774\n",
      "Loss at iter 593000: 61609.79474974215\n",
      "Loss at iter 594000: 61609.79474953498\n",
      "Loss at iter 595000: 61609.79474933503\n",
      "Loss at iter 596000: 61609.79474914199\n",
      "Loss at iter 597000: 61609.79474895568\n",
      "Loss at iter 598000: 61609.79474877585\n",
      "Loss at iter 599000: 61609.79474860225\n",
      "Loss at iter 600000: 61609.79474843469\n",
      "Loss at iter 601000: 61609.79474827294\n",
      "Loss at iter 602000: 61609.79474811682\n",
      "Loss at iter 603000: 61609.79474796612\n",
      "Loss at iter 604000: 61609.79474782067\n",
      "Loss at iter 605000: 61609.794747680244\n",
      "Loss at iter 606000: 61609.79474754472\n",
      "Loss at iter 607000: 61609.7947474139\n",
      "Loss at iter 608000: 61609.794747287626\n",
      "Loss at iter 609000: 61609.79474716573\n",
      "Loss at iter 610000: 61609.79474704807\n",
      "Loss at iter 611000: 61609.79474693451\n",
      "Loss at iter 612000: 61609.79474682489\n",
      "Loss at iter 613000: 61609.79474671908\n",
      "Loss at iter 614000: 61609.79474661693\n",
      "Loss at iter 615000: 61609.79474651835\n",
      "Loss at iter 616000: 61609.7947464232\n",
      "Loss at iter 617000: 61609.79474633133\n",
      "Loss at iter 618000: 61609.794746242675\n",
      "Loss at iter 619000: 61609.79474615708\n",
      "Loss at iter 620000: 61609.794746074476\n",
      "Loss at iter 621000: 61609.794745994725\n",
      "Loss at iter 622000: 61609.79474591776\n",
      "Loss at iter 623000: 61609.794745843465\n",
      "Loss at iter 624000: 61609.79474577175\n",
      "Loss at iter 625000: 61609.79474570252\n",
      "Loss at iter 626000: 61609.79474563569\n",
      "Loss at iter 627000: 61609.79474557121\n",
      "Loss at iter 628000: 61609.79474550893\n",
      "Loss at iter 629000: 61609.79474544885\n",
      "Loss at iter 630000: 61609.79474539085\n",
      "Loss at iter 631000: 61609.794745334846\n",
      "Loss at iter 632000: 61609.7947452808\n",
      "Loss at iter 633000: 61609.794745228646\n",
      "Loss at iter 634000: 61609.79474517829\n",
      "Loss at iter 635000: 61609.79474512968\n",
      "Loss at iter 636000: 61609.79474508276\n",
      "Loss at iter 637000: 61609.79474503748\n",
      "Loss at iter 638000: 61609.79474499375\n",
      "Loss at iter 639000: 61609.79474495157\n",
      "Loss at iter 640000: 61609.79474491084\n",
      "Loss at iter 641000: 61609.794744871506\n",
      "Loss at iter 642000: 61609.79474483357\n",
      "Loss at iter 643000: 61609.794744796935\n",
      "Loss at iter 644000: 61609.79474476157\n",
      "Loss at iter 645000: 61609.79474472745\n",
      "Loss at iter 646000: 61609.79474469451\n",
      "Loss at iter 647000: 61609.794744662715\n",
      "Loss at iter 648000: 61609.79474463202\n",
      "Loss at iter 649000: 61609.7947446024\n",
      "Loss at iter 650000: 61609.79474457378\n",
      "Loss at iter 651000: 61609.79474454619\n",
      "Loss at iter 652000: 61609.794744519546\n",
      "Loss at iter 653000: 61609.79474449382\n",
      "Loss at iter 654000: 61609.794744469\n",
      "Loss at iter 655000: 61609.79474444501\n",
      "Loss at iter 656000: 61609.794744421895\n",
      "Loss at iter 657000: 61609.794744399565\n",
      "Loss at iter 658000: 61609.79474437802\n",
      "Loss at iter 659000: 61609.79474435721\n",
      "Loss at iter 660000: 61609.79474433712\n",
      "Loss at iter 661000: 61609.79474431776\n",
      "Loss at iter 662000: 61609.79474429903\n",
      "Loss at iter 663000: 61609.79474428099\n",
      "Loss at iter 664000: 61609.794744263556\n",
      "Loss at iter 665000: 61609.79474424672\n",
      "Loss at iter 666000: 61609.79474423048\n",
      "Loss at iter 667000: 61609.794744214814\n",
      "Loss at iter 668000: 61609.79474419968\n",
      "Loss at iter 669000: 61609.79474418508\n",
      "Loss at iter 670000: 61609.79474417096\n",
      "Loss at iter 671000: 61609.79474415735\n",
      "Loss at iter 672000: 61609.79474414421\n",
      "Loss at iter 673000: 61609.794744131534\n",
      "Loss at iter 674000: 61609.794744119296\n",
      "Loss at iter 675000: 61609.79474410749\n",
      "Loss at iter 676000: 61609.79474409608\n",
      "Loss at iter 677000: 61609.79474408507\n",
      "Loss at iter 678000: 61609.79474407444\n",
      "Loss at iter 679000: 61609.794744064195\n",
      "Loss at iter 680000: 61609.794744054285\n",
      "Loss at iter 681000: 61609.79474404473\n",
      "Loss at iter 682000: 61609.79474403551\n",
      "Loss at iter 683000: 61609.794744026614\n",
      "Loss at iter 684000: 61609.79474401801\n",
      "Loss at iter 685000: 61609.79474400972\n",
      "Loss at iter 686000: 61609.794744001694\n",
      "Loss at iter 687000: 61609.79474399398\n",
      "Loss at iter 688000: 61609.79474398652\n",
      "Loss at iter 689000: 61609.79474397932\n",
      "Loss at iter 690000: 61609.79474397237\n",
      "Loss at iter 691000: 61609.79474396564\n",
      "Loss at iter 692000: 61609.79474395918\n",
      "Loss at iter 693000: 61609.79474395292\n",
      "Loss at iter 694000: 61609.79474394688\n",
      "Loss at iter 695000: 61609.79474394107\n",
      "Loss at iter 696000: 61609.79474393545\n",
      "Loss at iter 697000: 61609.79474393001\n",
      "Loss at iter 698000: 61609.794743924765\n",
      "Loss at iter 699000: 61609.794743919716\n",
      "Loss at iter 700000: 61609.79474391484\n",
      "Loss at iter 701000: 61609.79474391012\n",
      "Loss at iter 702000: 61609.794743905586\n",
      "Loss at iter 703000: 61609.7947439012\n",
      "Loss at iter 704000: 61609.794743896964\n",
      "Loss at iter 705000: 61609.794743892846\n",
      "Loss at iter 706000: 61609.79474388892\n",
      "Loss at iter 707000: 61609.794743885104\n",
      "Loss at iter 708000: 61609.79474388143\n",
      "Loss at iter 709000: 61609.79474387788\n",
      "Loss at iter 710000: 61609.79474387445\n",
      "Loss at iter 711000: 61609.79474387114\n",
      "Loss at iter 712000: 61609.79474386795\n",
      "Loss at iter 713000: 61609.79474386486\n",
      "Loss at iter 714000: 61609.79474386189\n",
      "Loss at iter 715000: 61609.79474385902\n",
      "Loss at iter 716000: 61609.79474385625\n",
      "Loss at iter 717000: 61609.79474385357\n",
      "Loss at iter 718000: 61609.794743850995\n",
      "Loss at iter 719000: 61609.79474384849\n",
      "Loss at iter 720000: 61609.79474384608\n",
      "Loss at iter 721000: 61609.79474384377\n",
      "Loss at iter 722000: 61609.79474384152\n",
      "Loss at iter 723000: 61609.79474383936\n",
      "Loss at iter 724000: 61609.79474383726\n",
      "Loss at iter 725000: 61609.794743835235\n",
      "Loss at iter 726000: 61609.7947438333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 727000: 61609.794743831415\n",
      "Loss at iter 728000: 61609.79474382961\n",
      "Loss at iter 729000: 61609.79474382787\n",
      "Loss at iter 730000: 61609.794743826176\n",
      "Loss at iter 731000: 61609.79474382455\n",
      "Loss at iter 732000: 61609.79474382297\n",
      "Loss at iter 733000: 61609.794743821454\n",
      "Loss at iter 734000: 61609.794743819984\n",
      "Loss at iter 735000: 61609.794743818566\n",
      "Loss at iter 736000: 61609.7947438172\n",
      "Loss at iter 737000: 61609.79474381589\n",
      "Loss at iter 738000: 61609.7947438146\n",
      "Loss at iter 739000: 61609.79474381338\n",
      "Loss at iter 740000: 61609.79474381221\n",
      "Loss at iter 741000: 61609.79474381103\n",
      "Loss at iter 742000: 61609.79474380994\n",
      "Loss at iter 743000: 61609.79474380887\n",
      "Loss at iter 744000: 61609.794743807855\n",
      "Loss at iter 745000: 61609.79474380685\n",
      "Loss at iter 746000: 61609.79474380589\n",
      "Loss at iter 747000: 61609.79474380497\n",
      "Loss at iter 748000: 61609.79474380407\n",
      "Loss at iter 749000: 61609.794743803206\n",
      "Loss at iter 750000: 61609.79474380236\n",
      "Loss at iter 751000: 61609.79474380158\n",
      "Loss at iter 752000: 61609.79474380079\n",
      "Loss at iter 753000: 61609.79474380005\n",
      "Loss at iter 754000: 61609.794743799335\n",
      "Loss at iter 755000: 61609.79474379863\n",
      "Loss at iter 756000: 61609.79474379796\n",
      "Loss at iter 757000: 61609.7947437973\n",
      "Loss at iter 758000: 61609.79474379668\n",
      "Loss at iter 759000: 61609.794743796054\n",
      "Loss at iter 760000: 61609.79474379549\n",
      "Loss at iter 761000: 61609.79474379491\n",
      "Loss at iter 762000: 61609.794743794366\n",
      "Loss at iter 763000: 61609.79474379385\n",
      "Loss at iter 764000: 61609.79474379334\n",
      "Loss at iter 765000: 61609.79474379285\n",
      "Loss at iter 766000: 61609.794743792365\n",
      "Loss at iter 767000: 61609.79474379192\n",
      "Loss at iter 768000: 61609.79474379147\n",
      "Loss at iter 769000: 61609.79474379105\n",
      "Loss at iter 770000: 61609.79474379063\n",
      "Loss at iter 771000: 61609.79474379024\n",
      "Loss at iter 772000: 61609.79474378986\n",
      "Loss at iter 773000: 61609.79474378949\n",
      "Loss at iter 774000: 61609.794743789134\n",
      "Loss at iter 775000: 61609.794743788785\n",
      "Loss at iter 776000: 61609.794743788465\n",
      "Loss at iter 777000: 61609.79474378813\n",
      "Loss at iter 778000: 61609.79474378783\n",
      "Loss at iter 779000: 61609.79474378752\n",
      "Loss at iter 780000: 61609.79474378724\n",
      "Loss at iter 781000: 61609.79474378696\n",
      "Loss at iter 782000: 61609.79474378669\n",
      "Loss at iter 783000: 61609.794743786435\n",
      "Loss at iter 784000: 61609.7947437862\n",
      "Loss at iter 785000: 61609.79474378594\n",
      "Loss at iter 786000: 61609.79474378572\n",
      "Loss at iter 787000: 61609.7947437855\n",
      "Loss at iter 788000: 61609.79474378526\n",
      "Loss at iter 789000: 61609.79474378505\n",
      "Loss at iter 790000: 61609.79474378486\n",
      "Loss at iter 791000: 61609.79474378465\n",
      "Loss at iter 792000: 61609.79474378448\n",
      "Loss at iter 793000: 61609.79474378429\n",
      "Loss at iter 794000: 61609.79474378411\n",
      "Loss at iter 795000: 61609.79474378395\n",
      "Loss at iter 796000: 61609.79474378378\n",
      "Loss at iter 797000: 61609.79474378361\n",
      "Loss at iter 798000: 61609.794743783474\n",
      "Loss at iter 799000: 61609.79474378333\n",
      "Loss at iter 800000: 61609.79474378317\n",
      "Loss at iter 801000: 61609.79474378304\n",
      "Loss at iter 802000: 61609.79474378291\n",
      "Loss at iter 803000: 61609.794743782775\n",
      "Loss at iter 804000: 61609.79474378266\n",
      "Loss at iter 805000: 61609.79474378254\n",
      "Loss at iter 806000: 61609.79474378244\n",
      "Loss at iter 807000: 61609.79474378231\n",
      "Loss at iter 808000: 61609.7947437822\n",
      "Loss at iter 809000: 61609.7947437821\n",
      "Loss at iter 810000: 61609.794743782004\n",
      "Loss at iter 811000: 61609.7947437819\n",
      "Loss at iter 812000: 61609.794743781815\n",
      "Loss at iter 813000: 61609.79474378173\n",
      "Loss at iter 814000: 61609.79474378164\n",
      "Loss at iter 815000: 61609.79474378155\n",
      "Loss at iter 816000: 61609.79474378147\n",
      "Loss at iter 817000: 61609.79474378139\n",
      "Loss at iter 818000: 61609.794743781305\n",
      "Loss at iter 819000: 61609.79474378125\n",
      "Loss at iter 820000: 61609.79474378117\n",
      "Loss at iter 821000: 61609.794743781116\n",
      "Loss at iter 822000: 61609.794743781036\n",
      "Loss at iter 823000: 61609.794743780985\n",
      "Loss at iter 824000: 61609.79474378091\n",
      "Loss at iter 825000: 61609.79474378086\n",
      "Loss at iter 826000: 61609.794743780796\n",
      "Loss at iter 827000: 61609.79474378075\n",
      "Loss at iter 828000: 61609.794743780694\n",
      "Loss at iter 829000: 61609.79474378064\n",
      "Loss at iter 830000: 61609.79474378059\n",
      "Loss at iter 831000: 61609.79474378055\n",
      "Loss at iter 832000: 61609.7947437805\n",
      "Loss at iter 833000: 61609.79474378047\n",
      "Loss at iter 834000: 61609.79474378039\n",
      "Loss at iter 835000: 61609.79474378037\n",
      "Loss at iter 836000: 61609.79474378032\n",
      "Loss at iter 837000: 61609.7947437803\n",
      "Loss at iter 838000: 61609.79474378026\n",
      "Loss at iter 839000: 61609.79474378022\n",
      "Loss at iter 840000: 61609.79474378018\n",
      "Loss at iter 841000: 61609.79474378016\n",
      "Loss at iter 842000: 61609.79474378012\n",
      "Loss at iter 843000: 61609.7947437801\n",
      "Loss at iter 844000: 61609.79474378007\n",
      "Loss at iter 845000: 61609.79474378003\n",
      "Loss at iter 846000: 61609.79474378\n",
      "Loss at iter 847000: 61609.79474377998\n",
      "Loss at iter 848000: 61609.794743779945\n",
      "Loss at iter 849000: 61609.79474377992\n",
      "Loss at iter 850000: 61609.794743779916\n",
      "Loss at iter 851000: 61609.79474377989\n",
      "Loss at iter 852000: 61609.79474377985\n",
      "Loss at iter 853000: 61609.79474377984\n",
      "Loss at iter 854000: 61609.794743779814\n",
      "Loss at iter 855000: 61609.7947437798\n",
      "Loss at iter 856000: 61609.794743779785\n",
      "Loss at iter 857000: 61609.79474377976\n",
      "Loss at iter 858000: 61609.794743779734\n",
      "Loss at iter 859000: 61609.79474377973\n",
      "Loss at iter 860000: 61609.79474377971\n",
      "Loss at iter 861000: 61609.79474377969\n",
      "Loss at iter 862000: 61609.794743779676\n",
      "Loss at iter 863000: 61609.79474377967\n",
      "Loss at iter 864000: 61609.79474377964\n",
      "Loss at iter 865000: 61609.794743779625\n",
      "Loss at iter 866000: 61609.79474377961\n",
      "Loss at iter 867000: 61609.7947437796\n",
      "Loss at iter 868000: 61609.79474377959\n",
      "Loss at iter 869000: 61609.79474377956\n",
      "Loss at iter 870000: 61609.79474377956\n",
      "Loss at iter 871000: 61609.79474377955\n",
      "Loss at iter 872000: 61609.79474377952\n",
      "Loss at iter 873000: 61609.79474377952\n",
      "Loss at iter 874000: 61609.79474377951\n",
      "Loss at iter 875000: 61609.7947437795\n",
      "Loss at iter 876000: 61609.794743779494\n",
      "Loss at iter 877000: 61609.79474377949\n",
      "Loss at iter 878000: 61609.79474377947\n",
      "Loss at iter 879000: 61609.79474377947\n",
      "Loss at iter 880000: 61609.79474377947\n",
      "Loss at iter 881000: 61609.79474377946\n",
      "Loss at iter 882000: 61609.79474377944\n",
      "Loss at iter 883000: 61609.79474377944\n",
      "Loss at iter 884000: 61609.794743779436\n",
      "Loss at iter 885000: 61609.79474377943\n",
      "Loss at iter 886000: 61609.79474377943\n",
      "Loss at iter 887000: 61609.79474377941\n",
      "Loss at iter 888000: 61609.7947437794\n",
      "Loss at iter 889000: 61609.79474377939\n",
      "Loss at iter 890000: 61609.7947437794\n",
      "Loss at iter 891000: 61609.794743779385\n",
      "Loss at iter 892000: 61609.79474377938\n",
      "Loss at iter 893000: 61609.794743779385\n",
      "Loss at iter 894000: 61609.79474377936\n",
      "Loss at iter 895000: 61609.79474377937\n",
      "Loss at iter 896000: 61609.79474377936\n",
      "Loss at iter 897000: 61609.794743779355\n",
      "Loss at iter 898000: 61609.79474377934\n",
      "Loss at iter 899000: 61609.79474377934\n",
      "Loss at iter 900000: 61609.794743779355\n",
      "Loss at iter 901000: 61609.79474377934\n",
      "Loss at iter 902000: 61609.794743779334\n",
      "Loss at iter 903000: 61609.794743779334\n",
      "Loss at iter 904000: 61609.794743779334\n",
      "Loss at iter 905000: 61609.79474377933\n",
      "Loss at iter 906000: 61609.794743779334\n",
      "Loss at iter 907000: 61609.794743779305\n",
      "Loss at iter 908000: 61609.79474377932\n",
      "Loss at iter 909000: 61609.79474377931\n",
      "Loss at iter 910000: 61609.794743779305\n",
      "Loss at iter 911000: 61609.794743779305\n",
      "Loss at iter 912000: 61609.794743779305\n",
      "Loss at iter 913000: 61609.7947437793\n",
      "Loss at iter 914000: 61609.794743779305\n",
      "Loss at iter 915000: 61609.794743779305\n",
      "Loss at iter 916000: 61609.79474377929\n",
      "Loss at iter 917000: 61609.79474377929\n",
      "Loss at iter 918000: 61609.79474377929\n",
      "Loss at iter 919000: 61609.7947437793\n",
      "Loss at iter 920000: 61609.7947437793\n",
      "Loss at iter 921000: 61609.794743779275\n",
      "Loss at iter 922000: 61609.794743779275\n",
      "Loss at iter 923000: 61609.79474377927\n",
      "Loss at iter 924000: 61609.79474377928\n",
      "Loss at iter 925000: 61609.794743779275\n",
      "Loss at iter 926000: 61609.79474377927\n",
      "Loss at iter 927000: 61609.79474377926\n",
      "Loss at iter 928000: 61609.794743779275\n",
      "Loss at iter 929000: 61609.79474377928\n",
      "Loss at iter 930000: 61609.794743779275\n",
      "Loss at iter 931000: 61609.79474377927\n",
      "Loss at iter 932000: 61609.794743779275\n",
      "Loss at iter 933000: 61609.79474377926\n",
      "Loss at iter 934000: 61609.79474377926\n",
      "Loss at iter 935000: 61609.79474377927\n",
      "Loss at iter 936000: 61609.794743779275\n",
      "Loss at iter 937000: 61609.79474377927\n",
      "Loss at iter 938000: 61609.79474377925\n",
      "Loss at iter 939000: 61609.794743779246\n",
      "Loss at iter 940000: 61609.79474377925\n",
      "Loss at iter 941000: 61609.79474377927\n",
      "Loss at iter 942000: 61609.79474377926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 943000: 61609.79474377926\n",
      "Loss at iter 944000: 61609.794743779246\n",
      "Loss at iter 945000: 61609.794743779246\n",
      "Loss at iter 946000: 61609.794743779246\n",
      "Loss at iter 947000: 61609.79474377924\n",
      "Loss at iter 948000: 61609.79474377925\n",
      "Loss at iter 949000: 61609.79474377924\n",
      "Loss at iter 950000: 61609.79474377925\n",
      "Loss at iter 951000: 61609.79474377925\n",
      "Loss at iter 952000: 61609.79474377925\n",
      "Loss at iter 953000: 61609.79474377926\n",
      "Loss at iter 954000: 61609.794743779246\n",
      "Loss at iter 955000: 61609.794743779246\n",
      "Loss at iter 956000: 61609.794743779246\n",
      "Loss at iter 957000: 61609.79474377925\n",
      "Loss at iter 958000: 61609.79474377924\n",
      "Loss at iter 959000: 61609.79474377924\n",
      "Loss at iter 960000: 61609.79474377924\n",
      "Loss at iter 961000: 61609.79474377925\n",
      "Loss at iter 962000: 61609.79474377924\n",
      "Loss at iter 963000: 61609.79474377923\n",
      "Loss at iter 964000: 61609.79474377924\n",
      "Loss at iter 965000: 61609.79474377924\n",
      "Loss at iter 966000: 61609.794743779246\n",
      "Loss at iter 967000: 61609.79474377926\n",
      "Loss at iter 968000: 61609.79474377923\n",
      "Loss at iter 969000: 61609.794743779246\n",
      "Loss at iter 970000: 61609.794743779246\n",
      "Loss at iter 971000: 61609.79474377924\n",
      "Loss at iter 972000: 61609.794743779246\n",
      "Loss at iter 973000: 61609.79474377924\n",
      "Loss at iter 974000: 61609.794743779225\n",
      "Loss at iter 975000: 61609.79474377924\n",
      "Loss at iter 976000: 61609.79474377923\n",
      "Loss at iter 977000: 61609.794743779246\n",
      "Loss at iter 978000: 61609.79474377924\n",
      "Loss at iter 979000: 61609.794743779225\n",
      "Loss at iter 980000: 61609.794743779225\n",
      "Loss at iter 981000: 61609.79474377924\n",
      "Loss at iter 982000: 61609.79474377923\n",
      "Loss at iter 983000: 61609.79474377924\n",
      "Loss at iter 984000: 61609.79474377924\n",
      "Loss at iter 985000: 61609.79474377925\n",
      "Loss at iter 986000: 61609.79474377923\n",
      "Loss at iter 987000: 61609.79474377924\n",
      "Loss at iter 988000: 61609.79474377924\n",
      "Loss at iter 989000: 61609.79474377923\n",
      "Loss at iter 990000: 61609.794743779225\n",
      "Loss at iter 991000: 61609.79474377923\n",
      "Loss at iter 992000: 61609.79474377923\n",
      "Loss at iter 993000: 61609.794743779225\n",
      "Loss at iter 994000: 61609.79474377924\n",
      "Loss at iter 995000: 61609.79474377924\n",
      "Loss at iter 996000: 61609.79474377924\n",
      "Loss at iter 997000: 61609.79474377924\n",
      "Loss at iter 998000: 61609.79474377923\n",
      "Loss at iter 999000: 61609.79474377923\n",
      "Final loss:  61609.794743779225\n",
      "Final W:  [[-130.02405553]\n",
      " [  59.42895228]]\n"
     ]
    }
   ],
   "source": [
    "#Khởi tạo lr\n",
    "np.random.seed(1)\n",
    "lr= 0.0001  \n",
    "#Khởi tạo W\n",
    "W=np.asarray([[60], [-144]])\n",
    "#Thêm một cột cho bias\n",
    "X=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1) \n",
    "print(\"shape của X là:\", X.shape)\n",
    "print()\n",
    "#GD\n",
    "for i in range(1000000):#gồm 10 vòng lặp\n",
    "    prediction=np.matmul(X,W) #Tính Y'=XW, Y' càng gần Y càng tốt\n",
    "    loss=prediction-y_train #Tính hàm loss(cost)\n",
    "    gradient=np.matmul(X.T,loss) #Tính Gradient Descent\n",
    "    W=W-lr*(1/X.shape[0])*gradient  #Cập nhật W\n",
    "    if i % 1000 == 0:\n",
    "        loss = np.mean(np.square(np.matmul(X,W)-y_train)) #Tính loss sau mỗi lần lặp để kiểm tra tính hội tụ\n",
    "        print(\"Loss at iter {}: {}\".format(i, loss)) #in ra giá trị loss thứ i\n",
    "    #save.append(W)\n",
    "loss=np.mean(np.square(np.matmul(X,W)-y_train)) \n",
    "print(\"Final loss: \", loss)\n",
    "print(\"Final W: \", W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# fig1=sns.scatterplot(x=\"Height\",y=\"Weight\",data=df)\n",
    "# fig2=sns.lineplot(x=df.loc[:,\"Height\"].values,y=np.matmul(X,W).reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape của X là: (127, 2)\n",
      "\n",
      "Loss at iter 0: 81953.42348836306\n",
      "Loss at iter 1000: 61804.384134170534\n",
      "Loss at iter 2000: 61631.17086672301\n",
      "Loss at iter 3000: 61631.08663346203\n",
      "Loss at iter 4000: 61618.385329171826\n",
      "Loss at iter 5000: 61696.01226207574\n",
      "Loss at iter 6000: 61614.95298981596\n",
      "Loss at iter 7000: 61633.413977705495\n",
      "Loss at iter 8000: 61628.686802157434\n",
      "Loss at iter 9000: 61625.69030195446\n",
      "Loss:  61716.34262602689\n",
      "W:  [[-129.81956273]\n",
      " [  58.37989051]]\n"
     ]
    }
   ],
   "source": [
    "#Khởi tạo lr\n",
    "lr=0.0001\n",
    "#Khởi tạo W\n",
    "W=np.asarray([[2], [1]])\n",
    "#Thêm một cột cho bias\n",
    "X=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1) \n",
    "print(\"shape của X là:\", X.shape)\n",
    "print()\n",
    "for i in range(10000):#gồm 100000 vòng lặp\n",
    "    rd_id = np.random.permutation(X.shape[0]) #Ta shuffle sau mỗi lần lặp\n",
    "    for j in rd_id:\n",
    "        X_1=X[j,:].reshape(1,-1)\n",
    "        error=np.matmul(X_1,W)-y_train[j,:].reshape(1,-1)\n",
    "        W=W-lr*np.matmul(X_1.T,error)\n",
    "    if i % 1000 == 0:\n",
    "        loss = np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "        print(\"Loss at iter {}: {}\".format(i, loss))\n",
    "    #save.append(W)\n",
    "loss=np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "print(\"Loss: \", loss)\n",
    "print(\"W: \", W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.   , 10.744])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape của X là: (127, 2)\n",
      "\n",
      "1\n",
      "Loss at iter 0: 233020.61417239983\n",
      "1\n",
      "Loss at iter 1000: 63894.043252323325\n",
      "1\n",
      "Loss at iter 2000: 63213.64572937032\n",
      "1\n",
      "Loss at iter 3000: 62735.91476721794\n",
      "1\n",
      "Loss at iter 4000: 62400.48310484428\n",
      "1\n",
      "Loss at iter 5000: 62164.964771228595\n",
      "1\n",
      "Loss at iter 6000: 61999.59908965699\n",
      "1\n",
      "Loss at iter 7000: 61883.49004531287\n",
      "1\n",
      "Loss at iter 8000: 61801.965813378025\n",
      "1\n",
      "Loss at iter 9000: 61744.7247911122\n",
      "1\n",
      "Loss at iter 10000: 61704.53386195809\n",
      "1\n",
      "Loss at iter 11000: 61676.314401118114\n",
      "1\n",
      "Loss at iter 12000: 61656.5005280164\n",
      "1\n",
      "Loss at iter 13000: 61642.58851284656\n",
      "1\n",
      "Loss at iter 14000: 61632.82039910665\n",
      "1\n",
      "Loss at iter 15000: 61625.96186388573\n",
      "1\n",
      "Loss at iter 16000: 61621.146245780874\n",
      "1\n",
      "Loss at iter 17000: 61617.76503142752\n",
      "1\n",
      "Loss at iter 18000: 61615.39096225211\n",
      "1\n",
      "Loss at iter 19000: 61613.724045026334\n",
      "1\n",
      "Loss at iter 20000: 61612.55364398394\n",
      "1\n",
      "Loss at iter 21000: 61611.73186436657\n",
      "1\n",
      "Loss at iter 22000: 61611.1548640777\n",
      "1\n",
      "Loss at iter 23000: 61610.7497319397\n",
      "1\n",
      "Loss at iter 24000: 61610.465274457754\n",
      "1\n",
      "Loss at iter 25000: 61610.26554688388\n",
      "1\n",
      "Loss at iter 26000: 61610.12531114991\n",
      "1\n",
      "Loss at iter 27000: 61610.02684672305\n",
      "1\n",
      "Loss at iter 28000: 61609.95771139585\n",
      "1\n",
      "Loss at iter 29000: 61609.90916905808\n",
      "1\n",
      "Loss at iter 30000: 61609.87508578035\n",
      "1\n",
      "Loss at iter 31000: 61609.85115471578\n",
      "1\n",
      "Loss at iter 32000: 61609.834351876096\n",
      "1\n",
      "Loss at iter 33000: 61609.82255401309\n",
      "1\n",
      "Loss at iter 34000: 61609.8142703197\n",
      "1\n",
      "Loss at iter 35000: 61609.80845404808\n",
      "1\n",
      "Loss at iter 36000: 61609.804370239806\n",
      "1\n",
      "Loss at iter 37000: 61609.80150285481\n",
      "1\n",
      "Loss at iter 38000: 61609.79948956322\n",
      "1\n",
      "Loss at iter 39000: 61609.7980759606\n",
      "1\n",
      "Loss at iter 40000: 61609.797083420635\n",
      "1\n",
      "Loss at iter 41000: 61609.796386523514\n",
      "1\n",
      "Loss at iter 42000: 61609.795897207594\n",
      "1\n",
      "Loss at iter 43000: 61609.795553641736\n",
      "1\n",
      "Loss at iter 44000: 61609.7953124121\n",
      "1\n",
      "Loss at iter 45000: 61609.79514303632\n",
      "1\n",
      "Loss at iter 46000: 61609.79502411163\n",
      "1\n",
      "Loss at iter 47000: 61609.79494061042\n",
      "1\n",
      "Loss at iter 48000: 61609.79488198132\n",
      "1\n",
      "Loss at iter 49000: 61609.79484081576\n",
      "1\n",
      "Loss at iter 50000: 61609.794811912\n",
      "1\n",
      "Loss at iter 51000: 61609.79479161763\n",
      "1\n",
      "Loss at iter 52000: 61609.794777368246\n",
      "1\n",
      "Loss at iter 53000: 61609.79476736324\n",
      "1\n",
      "Loss at iter 54000: 61609.7947603384\n",
      "1\n",
      "Loss at iter 55000: 61609.794755406\n",
      "1\n",
      "Loss at iter 56000: 61609.7947519428\n",
      "1\n",
      "Loss at iter 57000: 61609.794749511166\n",
      "1\n",
      "Loss at iter 58000: 61609.79474780381\n",
      "1\n",
      "Loss at iter 59000: 61609.79474660503\n",
      "1\n",
      "Loss at iter 60000: 61609.79474576333\n",
      "1\n",
      "Loss at iter 61000: 61609.79474517234\n",
      "1\n",
      "Loss at iter 62000: 61609.79474475737\n",
      "1\n",
      "Loss at iter 63000: 61609.794744466024\n",
      "1\n",
      "Loss at iter 64000: 61609.79474426146\n",
      "1\n",
      "Loss at iter 65000: 61609.79474411782\n",
      "1\n",
      "Loss at iter 66000: 61609.79474401696\n",
      "1\n",
      "Loss at iter 67000: 61609.79474394615\n",
      "1\n",
      "Loss at iter 68000: 61609.79474389643\n",
      "1\n",
      "Loss at iter 69000: 61609.794743861516\n",
      "1\n",
      "Loss at iter 70000: 61609.794743837\n",
      "1\n",
      "Loss at iter 71000: 61609.7947438198\n",
      "1\n",
      "Loss at iter 72000: 61609.79474380771\n",
      "1\n",
      "Loss at iter 73000: 61609.79474379924\n",
      "1\n",
      "Loss at iter 74000: 61609.79474379327\n",
      "1\n",
      "Loss at iter 75000: 61609.79474378908\n",
      "1\n",
      "Loss at iter 76000: 61609.79474378615\n",
      "1\n",
      "Loss at iter 77000: 61609.794743784085\n",
      "1\n",
      "Loss at iter 78000: 61609.79474378264\n",
      "1\n",
      "Loss at iter 79000: 61609.79474378163\n",
      "1\n",
      "Loss at iter 80000: 61609.79474378091\n",
      "1\n",
      "Loss at iter 81000: 61609.79474378041\n",
      "1\n",
      "Loss at iter 82000: 61609.794743780054\n",
      "1\n",
      "Loss at iter 83000: 61609.79474377982\n",
      "1\n",
      "Loss at iter 84000: 61609.79474377964\n",
      "1\n",
      "Loss at iter 85000: 61609.79474377951\n",
      "1\n",
      "Loss at iter 86000: 61609.79474377944\n",
      "1\n",
      "Loss at iter 87000: 61609.79474377937\n",
      "1\n",
      "Loss at iter 88000: 61609.794743779334\n",
      "1\n",
      "Loss at iter 89000: 61609.79474377928\n",
      "1\n",
      "Loss at iter 90000: 61609.79474377929\n",
      "1\n",
      "Loss at iter 91000: 61609.79474377927\n",
      "1\n",
      "Loss at iter 92000: 61609.79474377925\n",
      "1\n",
      "Loss at iter 93000: 61609.79474377926\n",
      "1\n",
      "Loss at iter 94000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 95000: 61609.794743779246\n",
      "1\n",
      "Loss at iter 96000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 97000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 98000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 99000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 100000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 101000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 102000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 103000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 104000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 105000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 106000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 107000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 108000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 109000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 110000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 111000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 112000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 113000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 114000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 115000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 116000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 117000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 118000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 119000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 120000: 61609.794743779246\n",
      "1\n",
      "Loss at iter 121000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 122000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 123000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 124000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 125000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 126000: 61609.79474377922\n",
      "1\n",
      "Loss at iter 127000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 128000: 61609.79474377922\n",
      "1\n",
      "Loss at iter 129000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 130000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 131000: 61609.79474377922\n",
      "1\n",
      "Loss at iter 132000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 133000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 134000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 135000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 136000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 137000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 138000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 139000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 140000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 141000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 142000: 61609.794743779246\n",
      "1\n",
      "Loss at iter 143000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 144000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 145000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 146000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 147000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 148000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 149000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 150000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 151000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 152000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 153000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 154000: 61609.79474377921\n",
      "1\n",
      "Loss at iter 155000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 156000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 157000: 61609.794743779225\n",
      "1\n",
      "Loss at iter 158000: 61609.79474377923\n",
      "1\n",
      "Loss at iter 159000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 160000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 161000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 162000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 163000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 164000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 165000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 166000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 167000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 168000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 169000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 170000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 171000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 172000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 173000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 174000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 175000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 176000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 177000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 178000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 179000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 180000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 181000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 182000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 183000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 184000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 185000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 186000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 187000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 188000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 189000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 190000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 191000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 192000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 193000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 194000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 195000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 196000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 197000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 198000: 61609.79474377924\n",
      "1\n",
      "Loss at iter 199000: 61609.79474377924\n",
      "Loss:  61609.79474377924\n",
      "W:  [[-130.02405987]\n",
      " [  59.42895267]]\n"
     ]
    }
   ],
   "source": [
    "#Khởi tạo W\n",
    "lr=0.001\n",
    "W=np.asarray([[1.0], [2.0]])\n",
    "#Thêm một cột cho bias\n",
    "X=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1) \n",
    "first_X=X.copy()\n",
    "first_y=y_train.copy()\n",
    "print(\"shape của X là:\", X.shape)\n",
    "print()\n",
    "for i in range(200000):#gồm 100000 vòng lặp\n",
    "    indices = np.random.permutation(X.shape[0]) #Ta shuffle sau mỗi lần lặp\n",
    "    first_X=first_X[indices]\n",
    "    first_y=first_y[indices]\n",
    "\n",
    "    temp=10\n",
    "    z=0\n",
    "    for j in range(0,X.shape[0],128):\n",
    "        z+=1\n",
    "        X_1=first_X[j:temp,:]\n",
    "        prediction=np.matmul(X_1,W)\n",
    "        loss=prediction-first_y[j:temp,:]\n",
    "        gradient=np.matmul(X_1.T,loss)\n",
    "        W=W-lr*(1/X_1.shape[0])*gradient\n",
    "\n",
    "        temp=temp+ 128 if X.shape[0]-temp>=128 else X.shape[0]\n",
    "    if i % 1000 == 0:\n",
    "        print(z)\n",
    "        loss = np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "        print(\"Loss at iter {}: {}\".format(i, loss))\n",
    "    #save.append(W)\n",
    "loss=np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "print(\"Loss: \", loss)\n",
    "print(\"W: \", W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Câu 2: Dùng 3 thuật toán Gradient Descent đã học để tìm bộ Weight theo Height và Width trong data fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 2)\n",
      "(127, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df.loc[:,[\"Height\",\"Width\"]].values,df.loc[:,\"Weight\"].values,test_size=0.2,random_state=1)\n",
    "y_train=y_train.reshape(-1,1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveW = []\n",
    "saveloss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape của X là: (127, 3)\n",
      "\n",
      "Loss at iter 0: 254634.3236565197\n",
      "Loss at iter 1000: 55808.963727947594\n",
      "Loss at iter 2000: 53424.48118464908\n",
      "Loss at iter 3000: 51475.69389767256\n",
      "Loss at iter 4000: 49863.78473022939\n",
      "Loss at iter 5000: 48512.949866440846\n",
      "Loss at iter 6000: 47365.024119685906\n",
      "Loss at iter 7000: 46375.36185856839\n",
      "Loss at iter 8000: 45509.68020601981\n",
      "Loss at iter 9000: 44741.639702215274\n",
      "Loss at iter 10000: 44050.99014286947\n",
      "Loss at iter 11000: 43422.14955521532\n",
      "Loss at iter 12000: 42843.11512115474\n",
      "Loss at iter 13000: 42304.62849760775\n",
      "Loss at iter 14000: 41799.536101618665\n",
      "Loss at iter 15000: 41322.298812617184\n",
      "Loss at iter 16000: 40868.616185235805\n",
      "Loss at iter 17000: 40435.13842109201\n",
      "Loss at iter 18000: 40019.24559774414\n",
      "Loss at iter 19000: 39618.878442725814\n",
      "Loss at iter 20000: 39232.40861127027\n",
      "Loss at iter 21000: 38858.53923947742\n",
      "Loss at iter 22000: 38496.22870060025\n",
      "Loss at iter 23000: 38144.632144374904\n",
      "Loss at iter 24000: 37803.056665562006\n",
      "Loss at iter 25000: 37470.92691828561\n",
      "Loss at iter 26000: 37147.75873646107\n",
      "Loss at iter 27000: 36833.13889056229\n",
      "Loss at iter 28000: 36526.70954778103\n",
      "Loss at iter 29000: 36228.156337386026\n",
      "Loss at iter 30000: 35937.19917963905\n",
      "Loss at iter 31000: 35653.585233237834\n",
      "Loss at iter 32000: 35377.08346693541\n",
      "Loss at iter 33000: 35107.480476463825\n",
      "Loss at iter 34000: 34844.57725638941\n",
      "Loss at iter 35000: 34588.18670435116\n",
      "Loss at iter 36000: 34338.1316871135\n",
      "Loss at iter 37000: 34094.24353770037\n",
      "Loss at iter 38000: 33856.360883406946\n",
      "Loss at iter 39000: 33624.32872788382\n",
      "Loss at iter 40000: 33397.9977284184\n",
      "Loss at iter 41000: 33177.2236232833\n",
      "Loss at iter 42000: 32961.866774550916\n",
      "Loss at iter 43000: 32751.791799847593\n",
      "Loss at iter 44000: 32546.867272706844\n",
      "Loss at iter 45000: 32346.965475921206\n",
      "Loss at iter 46000: 32151.96219592809\n",
      "Loss at iter 47000: 31961.73654904862\n",
      "Loss at iter 48000: 31776.170832534728\n",
      "Loss at iter 49000: 31595.150395014676\n",
      "Loss at iter 50000: 31418.563522182427\n",
      "Loss at iter 51000: 31246.30133453726\n",
      "Loss at iter 52000: 31078.257694717227\n",
      "Loss at iter 53000: 30914.329122535197\n",
      "Loss at iter 54000: 30754.414716259504\n",
      "Loss at iter 55000: 30598.416079013125\n",
      "Loss at iter 56000: 30446.237249420938\n",
      "Loss at iter 57000: 30297.784635829295\n",
      "Loss at iter 58000: 30152.966953572803\n",
      "Loss at iter 59000: 30011.69516487776\n",
      "Loss at iter 60000: 29873.882421080983\n",
      "Loss at iter 61000: 29739.444006909718\n",
      "Loss at iter 62000: 29608.297286621546\n",
      "Loss at iter 63000: 29480.361651842606\n",
      "Loss at iter 64000: 29355.558470974058\n",
      "Loss at iter 65000: 29233.81104006051\n",
      "Loss at iter 66000: 29115.044535032048\n",
      "Loss at iter 67000: 28999.185965246787\n",
      "Loss at iter 68000: 28886.164128271288\n",
      "Loss at iter 69000: 28775.909565844944\n",
      "Loss at iter 70000: 28668.35452098175\n",
      "Loss at iter 71000: 28563.432896167393\n",
      "Loss at iter 72000: 28461.08021261452\n",
      "Loss at iter 73000: 28361.23357054233\n",
      "Loss at iter 74000: 28263.831610448815\n",
      "Loss at iter 75000: 28168.814475346895\n",
      "Loss at iter 76000: 28076.123773937205\n",
      "Loss at iter 77000: 27985.702544691623\n",
      "Loss at iter 78000: 27897.49522082316\n",
      "Loss at iter 79000: 27811.447596119153\n",
      "Loss at iter 80000: 27727.506791614876\n",
      "Loss at iter 81000: 27645.621223086393\n",
      "Loss at iter 82000: 27565.740569341728\n",
      "Loss at iter 83000: 27487.815741290313\n",
      "Loss at iter 84000: 27411.7988517711\n",
      "Loss at iter 85000: 27337.643186120495\n",
      "Loss at iter 86000: 27265.303173461933\n",
      "Loss at iter 87000: 27194.734358698628\n",
      "Loss at iter 88000: 27125.893375193136\n",
      "Loss at iter 89000: 27058.737918115727\n",
      "Loss at iter 90000: 26993.22671844579\n",
      "Loss at iter 91000: 26929.31951760999\n",
      "Loss at iter 92000: 26866.977042741757\n",
      "Loss at iter 93000: 26806.160982546124\n",
      "Loss at iter 94000: 26746.833963756293\n",
      "Loss at iter 95000: 26688.959528166237\n",
      "Loss at iter 96000: 26632.502110225778\n",
      "Loss at iter 97000: 26577.42701518444\n",
      "Loss at iter 98000: 26523.700397770375\n",
      "Loss at iter 99000: 26471.289241391307\n",
      "Loss at iter 100000: 26420.161337844842\n",
      "Loss at iter 101000: 26370.28526752548\n",
      "Loss at iter 102000: 26321.63038011627\n",
      "Loss at iter 103000: 26274.166775753267\n",
      "Loss at iter 104000: 26227.865286651304\n",
      "Loss at iter 105000: 26182.69745917928\n",
      "Loss at iter 106000: 26138.635536374648\n",
      "Loss at iter 107000: 26095.65244088605\n",
      "Loss at iter 108000: 26053.721758333246\n",
      "Loss at iter 109000: 26012.81772107488\n",
      "Loss at iter 110000: 25972.91519237339\n",
      "Loss at iter 111000: 25933.989650947784\n",
      "Loss at iter 112000: 25896.01717590462\n",
      "Loss at iter 113000: 25858.974432037834\n",
      "Loss at iter 114000: 25822.838655488576\n",
      "Loss at iter 115000: 25787.58763975621\n",
      "Loss at iter 116000: 25753.19972205177\n",
      "Loss at iter 117000: 25719.653769985332\n",
      "Loss at iter 118000: 25686.9291685799\n",
      "Loss at iter 119000: 25655.005807602563\n",
      "Loss at iter 120000: 25623.86406920627\n",
      "Loss at iter 121000: 25593.484815874217\n",
      "Loss at iter 122000: 25563.849378659062\n",
      "Loss at iter 123000: 25534.939545710437\n",
      "Loss at iter 124000: 25506.73755108321\n",
      "Loss at iter 125000: 25479.226063819762\n",
      "Loss at iter 126000: 25452.388177299657\n",
      "Loss at iter 127000: 25426.207398849914\n",
      "Loss at iter 128000: 25400.66763960995\n",
      "Loss at iter 129000: 25375.753204644305\n",
      "Loss at iter 130000: 25351.44878329776\n",
      "Loss at iter 131000: 25327.739439786463\n",
      "Loss at iter 132000: 25304.610604019435\n",
      "Loss at iter 133000: 25282.04806264485\n",
      "Loss at iter 134000: 25260.03795031548\n",
      "Loss at iter 135000: 25238.566741168044\n",
      "Loss at iter 136000: 25217.621240511147\n",
      "Loss at iter 137000: 25197.188576716788\n",
      "Loss at iter 138000: 25177.256193310302\n",
      "Loss at iter 139000: 25157.811841254195\n",
      "Loss at iter 140000: 25138.84357142048\n",
      "Loss at iter 141000: 25120.33972724785\n",
      "Loss at iter 142000: 25102.288937578207\n",
      "Loss at iter 143000: 25084.68010966887\n",
      "Loss at iter 144000: 25067.502422375634\n",
      "Loss at iter 145000: 25050.745319502963\n",
      "Loss at iter 146000: 25034.39850331686\n",
      "Loss at iter 147000: 25018.451928216502\n",
      "Loss at iter 148000: 25002.895794561144\n",
      "Loss at iter 149000: 24987.720542647796\n",
      "Loss at iter 150000: 24972.91684683647\n",
      "Loss at iter 151000: 24958.475609819445\n",
      "Loss at iter 152000: 24944.38795703059\n",
      "Loss at iter 153000: 24930.64523119165\n",
      "Loss at iter 154000: 24917.23898699225\n",
      "Loss at iter 155000: 24904.16098589986\n",
      "Loss at iter 156000: 24891.403191097026\n",
      "Loss at iter 157000: 24878.957762542537\n",
      "Loss at iter 158000: 24866.817052153387\n",
      "Loss at iter 159000: 24854.97359910496\n",
      "Loss at iter 160000: 24843.420125245957\n",
      "Loss at iter 161000: 24832.149530625913\n",
      "Loss at iter 162000: 24821.154889131965\n",
      "Loss at iter 163000: 24810.429444232577\n",
      "Loss at iter 164000: 24799.966604825502\n",
      "Loss at iter 165000: 24789.75994118727\n",
      "Loss at iter 166000: 24779.803181022013\n",
      "Loss at iter 167000: 24770.090205606895\n",
      "Loss at iter 168000: 24760.61504603199\n",
      "Loss at iter 169000: 24751.37187953228\n",
      "Loss at iter 170000: 24742.355025909237\n",
      "Loss at iter 171000: 24733.558944040396\n",
      "Loss at iter 172000: 24724.97822847393\n",
      "Loss at iter 173000: 24716.607606107034\n",
      "Loss at iter 174000: 24708.441932945338\n",
      "Loss at iter 175000: 24700.47619094181\n",
      "Loss at iter 176000: 24692.705484912996\n",
      "Loss at iter 177000: 24685.125039530834\n",
      "Loss at iter 178000: 24677.73019638804\n",
      "Loss at iter 179000: 24670.516411135355\n",
      "Loss at iter 180000: 24663.479250688943\n",
      "Loss at iter 181000: 24656.61439050614\n",
      "Loss at iter 182000: 24649.91761192786\n",
      "Loss at iter 183000: 24643.384799586125\n",
      "Loss at iter 184000: 24637.011938875043\n",
      "Loss at iter 185000: 24630.795113483706\n",
      "Loss at iter 186000: 24624.730502989547\n",
      "Loss at iter 187000: 24618.814380510514\n",
      "Loss at iter 188000: 24613.043110414823\n",
      "Loss at iter 189000: 24607.413146086765\n",
      "Loss at iter 190000: 24601.92102774717\n",
      "Loss at iter 191000: 24596.563380327323\n",
      "Loss at iter 192000: 24591.336911394847\n",
      "Loss at iter 193000: 24586.238409130416\n",
      "Loss at iter 194000: 24581.26474035398\n",
      "Loss at iter 195000: 24576.412848599375\n",
      "Loss at iter 196000: 24571.679752235996\n",
      "Loss at iter 197000: 24567.062542636493\n",
      "Loss at iter 198000: 24562.558382389372\n",
      "Loss at iter 199000: 24558.164503555294\n",
      "Loss at iter 200000: 24553.87820596613\n",
      "Loss at iter 201000: 24549.69685556556\n",
      "Loss at iter 202000: 24545.6178827904\n",
      "Loss at iter 203000: 24541.638780991576\n",
      "Loss at iter 204000: 24537.75710489365\n",
      "Loss at iter 205000: 24533.97046909215\n",
      "Loss at iter 206000: 24530.27654658769\n",
      "Loss at iter 207000: 24526.67306735594\n",
      "Loss at iter 208000: 24523.15781695259\n",
      "Loss at iter 209000: 24519.728635152587\n",
      "Loss at iter 210000: 24516.383414622553\n",
      "Loss at iter 211000: 24513.120099625805\n",
      "Loss at iter 212000: 24509.936684759003\n",
      "Loss at iter 213000: 24506.831213719815\n",
      "Loss at iter 214000: 24503.80177810468\n",
      "Loss at iter 215000: 24500.84651623605\n",
      "Loss at iter 216000: 24497.963612018324\n",
      "Loss at iter 217000: 24495.151293821837\n",
      "Loss at iter 218000: 24492.40783339409\n",
      "Loss at iter 219000: 24489.731544797716\n",
      "Loss at iter 220000: 24487.12078337442\n",
      "Loss at iter 221000: 24484.573944734322\n",
      "Loss at iter 222000: 24482.089463769928\n",
      "Loss at iter 223000: 24479.665813694402\n",
      "Loss at iter 224000: 24477.301505103296\n",
      "Loss at iter 225000: 24474.995085059207\n",
      "Loss at iter 226000: 24472.745136199006\n",
      "Loss at iter 227000: 24470.550275862737\n",
      "Loss at iter 228000: 24468.40915524399\n",
      "Loss at iter 229000: 24466.320458560975\n",
      "Loss at iter 230000: 24464.282902247985\n",
      "Loss at iter 231000: 24462.295234166537\n",
      "Loss at iter 232000: 24460.356232835984\n",
      "Loss at iter 233000: 24458.4647066828\n",
      "Loss at iter 234000: 24456.61949330838\n",
      "Loss at iter 235000: 24454.819458774655\n",
      "Loss at iter 236000: 24453.063496907318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 237000: 24451.350528615985\n",
      "Loss at iter 238000: 24449.679501231145\n",
      "Loss at iter 239000: 24448.049387857154\n",
      "Loss at iter 240000: 24446.459186741275\n",
      "Loss at iter 241000: 24444.90792065799\n",
      "Loss at iter 242000: 24443.394636308545\n",
      "Loss at iter 243000: 24441.918403735075\n",
      "Loss at iter 244000: 24440.47831574911\n",
      "Loss at iter 245000: 24439.073487374102\n",
      "Loss at iter 246000: 24437.70305530158\n",
      "Loss at iter 247000: 24436.366177360607\n",
      "Loss at iter 248000: 24435.062032000245\n",
      "Loss at iter 249000: 24433.78981778472\n",
      "Loss at iter 250000: 24432.548752900846\n",
      "Loss at iter 251000: 24431.338074677664\n",
      "Loss at iter 252000: 24430.15703911767\n",
      "Loss at iter 253000: 24429.004920439693\n",
      "Loss at iter 254000: 24427.881010632806\n",
      "Loss at iter 255000: 24426.784619021288\n",
      "Loss at iter 256000: 24425.715071840146\n",
      "Loss at iter 257000: 24424.671711821105\n",
      "Loss at iter 258000: 24423.653897788623\n",
      "Loss at iter 259000: 24422.66100426598\n",
      "Loss at iter 260000: 24421.69242109081\n",
      "Loss at iter 261000: 24420.747553040186\n",
      "Loss at iter 262000: 24419.825819464815\n",
      "Loss at iter 263000: 24418.926653932223\n",
      "Loss at iter 264000: 24418.04950387866\n",
      "Loss at iter 265000: 24417.193830269567\n",
      "Loss at iter 266000: 24416.35910726825\n",
      "Loss at iter 267000: 24415.54482191281\n",
      "Loss at iter 268000: 24414.750473800865\n",
      "Loss at iter 269000: 24413.97557478207\n",
      "Loss at iter 270000: 24413.21964865815\n",
      "Loss at iter 271000: 24412.482230890157\n",
      "Loss at iter 272000: 24411.762868313162\n",
      "Loss at iter 273000: 24411.061118857608\n",
      "Loss at iter 274000: 24410.37655127777\n",
      "Loss at iter 275000: 24409.70874488668\n",
      "Loss at iter 276000: 24409.057289297612\n",
      "Loss at iter 277000: 24408.421784171867\n",
      "Loss at iter 278000: 24407.80183897283\n",
      "Loss at iter 279000: 24407.19707272588\n",
      "Loss at iter 280000: 24406.6071137843\n",
      "Loss at iter 281000: 24406.03159960094\n",
      "Loss at iter 282000: 24405.470176505347\n",
      "Loss at iter 283000: 24404.922499486474\n",
      "Loss at iter 284000: 24404.388231980643\n",
      "Loss at iter 285000: 24403.86704566469\n",
      "Loss at iter 286000: 24403.35862025426\n",
      "Loss at iter 287000: 24402.862643306955\n",
      "Loss at iter 288000: 24402.378810030295\n",
      "Loss at iter 289000: 24401.906823094454\n",
      "Loss at iter 290000: 24401.446392449558\n",
      "Loss at iter 291000: 24400.997235147377\n",
      "Loss at iter 292000: 24400.559075167497\n",
      "Loss at iter 293000: 24400.131643247696\n",
      "Loss at iter 294000: 24399.71467671845\n",
      "Loss at iter 295000: 24399.307919341532\n",
      "Loss at iter 296000: 24398.91112115252\n",
      "Loss at iter 297000: 24398.52403830723\n",
      "Loss at iter 298000: 24398.146432931833\n",
      "Loss at iter 299000: 24397.778072976693\n",
      "Loss at iter 300000: 24397.418732073747\n",
      "Loss at iter 301000: 24397.06818939742\n",
      "Loss at iter 302000: 24396.72622952888\n",
      "Loss at iter 303000: 24396.392642323717\n",
      "Loss at iter 304000: 24396.067222782727\n",
      "Loss at iter 305000: 24395.74977092603\n",
      "Loss at iter 306000: 24395.44009167009\n",
      "Loss at iter 307000: 24395.137994707864\n",
      "Loss at iter 308000: 24394.843294391867\n",
      "Loss at iter 309000: 24394.555809620048\n",
      "Loss at iter 310000: 24394.275363724537\n",
      "Loss at iter 311000: 24394.00178436307\n",
      "Loss at iter 312000: 24393.734903413042\n",
      "Loss at iter 313000: 24393.474556868252\n",
      "Loss at iter 314000: 24393.22058473807\n",
      "Loss at iter 315000: 24392.972830949107\n",
      "Loss at iter 316000: 24392.731143249377\n",
      "Loss at iter 317000: 24392.495373114638\n",
      "Loss at iter 318000: 24392.26537565721\n",
      "Loss at iter 319000: 24392.041009536864\n",
      "Loss at iter 320000: 24391.822136873987\n",
      "Loss at iter 321000: 24391.60862316488\n",
      "Loss at iter 322000: 24391.40033719908\n",
      "Loss at iter 323000: 24391.197150978704\n",
      "Loss at iter 324000: 24390.998939639827\n",
      "Loss at iter 325000: 24390.805581375735\n",
      "Loss at iter 326000: 24390.616957362083\n",
      "Loss at iter 327000: 24390.432951683826\n",
      "Loss at iter 328000: 24390.25345126407\n",
      "Loss at iter 329000: 24390.078345794507\n",
      "Loss at iter 330000: 24389.90752766766\n",
      "Loss at iter 331000: 24389.740891910737\n",
      "Loss at iter 332000: 24389.57833612118\n",
      "Loss at iter 333000: 24389.419760403627\n",
      "Loss at iter 334000: 24389.26506730864\n",
      "Loss at iter 335000: 24389.11416177275\n",
      "Loss at iter 336000: 24388.966951060054\n",
      "Loss at iter 337000: 24388.823344705215\n",
      "Loss at iter 338000: 24388.68325445791\n",
      "Loss at iter 339000: 24388.546594228534\n",
      "Loss at iter 340000: 24388.413280035344\n",
      "Loss at iter 341000: 24388.28322995285\n",
      "Loss at iter 342000: 24388.15636406143\n",
      "Loss at iter 343000: 24388.03260439825\n",
      "Loss at iter 344000: 24387.91187490936\n",
      "Loss at iter 345000: 24387.794101402917\n",
      "Loss at iter 346000: 24387.67921150362\n",
      "Loss at iter 347000: 24387.567134608238\n",
      "Loss at iter 348000: 24387.457801842243\n",
      "Loss at iter 349000: 24387.351146017387\n",
      "Loss at iter 350000: 24387.247101590525\n",
      "Loss at iter 351000: 24387.14560462331\n",
      "Loss at iter 352000: 24387.046592742845\n",
      "Loss at iter 353000: 24386.950005103412\n",
      "Loss at iter 354000: 24386.85578234906\n",
      "Loss at iter 355000: 24386.763866577126\n",
      "Loss at iter 356000: 24386.67420130265\n",
      "Loss at iter 357000: 24386.58673142368\n",
      "Loss at iter 358000: 24386.501403187398\n",
      "Loss at iter 359000: 24386.418164157098\n",
      "Loss at iter 360000: 24386.3369631799\n",
      "Loss at iter 361000: 24386.257750355453\n",
      "Loss at iter 362000: 24386.180477005106\n",
      "Loss at iter 363000: 24386.105095642146\n",
      "Loss at iter 364000: 24386.03155994246\n",
      "Loss at iter 365000: 24385.959824716218\n",
      "Loss at iter 366000: 24385.889845879992\n",
      "Loss at iter 367000: 24385.821580429732\n",
      "Loss at iter 368000: 24385.754986414297\n",
      "Loss at iter 369000: 24385.690022909712\n",
      "Loss at iter 370000: 24385.626649993977\n",
      "Loss at iter 371000: 24385.564828722563\n",
      "Loss at iter 372000: 24385.504521104493\n",
      "Loss at iter 373000: 24385.445690078945\n",
      "Loss at iter 374000: 24385.38829949251\n",
      "Loss at iter 375000: 24385.33231407699\n",
      "Loss at iter 376000: 24385.277699427694\n",
      "Loss at iter 377000: 24385.224421982315\n",
      "Loss at iter 378000: 24385.172449000274\n",
      "Loss at iter 379000: 24385.121748542657\n",
      "Loss at iter 380000: 24385.07228945253\n",
      "Loss at iter 381000: 24385.024041335844\n",
      "Loss at iter 382000: 24384.97697454266\n",
      "Loss at iter 383000: 24384.93106014908\n",
      "Loss at iter 384000: 24384.886269939358\n",
      "Loss at iter 385000: 24384.842576388575\n",
      "Loss at iter 386000: 24384.799952645764\n",
      "Loss at iter 387000: 24384.75837251737\n",
      "Loss at iter 388000: 24384.717810451188\n",
      "Loss at iter 389000: 24384.67824152062\n",
      "Loss at iter 390000: 24384.639641409394\n",
      "Loss at iter 391000: 24384.60198639661\n",
      "Loss at iter 392000: 24384.56525334214\n",
      "Loss at iter 393000: 24384.529419672457\n",
      "Loss at iter 394000: 24384.494463366686\n",
      "Loss at iter 395000: 24384.460362943173\n",
      "Loss at iter 396000: 24384.42709744619\n",
      "Loss at iter 397000: 24384.394646433106\n",
      "Loss at iter 398000: 24384.36298996182\n",
      "Loss at iter 399000: 24384.332108578496\n",
      "Loss at iter 400000: 24384.301983305617\n",
      "Loss at iter 401000: 24384.272595630307\n",
      "Loss at iter 402000: 24384.24392749297\n",
      "Loss at iter 403000: 24384.2159612762\n",
      "Loss at iter 404000: 24384.18867979393\n",
      "Loss at iter 405000: 24384.16206628088\n",
      "Loss at iter 406000: 24384.136104382258\n",
      "Loss at iter 407000: 24384.110778143724\n",
      "Loss at iter 408000: 24384.086072001563\n",
      "Loss at iter 409000: 24384.06197077308\n",
      "Loss at iter 410000: 24384.038459647396\n",
      "Loss at iter 411000: 24384.015524176233\n",
      "Loss at iter 412000: 24383.99315026505\n",
      "Loss at iter 413000: 24383.97132416442\n",
      "Loss at iter 414000: 24383.950032461573\n",
      "Loss at iter 415000: 24383.92926207213\n",
      "Loss at iter 416000: 24383.909000232055\n",
      "Loss at iter 417000: 24383.889234489863\n",
      "Loss at iter 418000: 24383.869952698922\n",
      "Loss at iter 419000: 24383.85114301\n",
      "Loss at iter 420000: 24383.832793863985\n",
      "Loss at iter 421000: 24383.814893984792\n",
      "Loss at iter 422000: 24383.797432372423\n",
      "Loss at iter 423000: 24383.780398296192\n",
      "Loss at iter 424000: 24383.763781288155\n",
      "Loss at iter 425000: 24383.74757113667\n",
      "Loss at iter 426000: 24383.731757880134\n",
      "Loss at iter 427000: 24383.716331800828\n",
      "Loss at iter 428000: 24383.701283418955\n",
      "Loss at iter 429000: 24383.68660348686\n",
      "Loss at iter 430000: 24383.67228298328\n",
      "Loss at iter 431000: 24383.65831310786\n",
      "Loss at iter 432000: 24383.644685275685\n",
      "Loss at iter 433000: 24383.631391112056\n",
      "Loss at iter 434000: 24383.618422447304\n",
      "Loss at iter 435000: 24383.60577131182\n",
      "Loss at iter 436000: 24383.593429931105\n",
      "Loss at iter 437000: 24383.581390721007\n",
      "Loss at iter 438000: 24383.56964628308\n",
      "Loss at iter 439000: 24383.55818940002\n",
      "Loss at iter 440000: 24383.547013031242\n",
      "Loss at iter 441000: 24383.536110308534\n",
      "Loss at iter 442000: 24383.52547453185\n",
      "Loss at iter 443000: 24383.515099165183\n",
      "Loss at iter 444000: 24383.504977832574\n",
      "Loss at iter 445000: 24383.495104314177\n",
      "Loss at iter 446000: 24383.48547254239\n",
      "Loss at iter 447000: 24383.47607659823\n",
      "Loss at iter 448000: 24383.46691070761\n",
      "Loss at iter 449000: 24383.457969237832\n",
      "Loss at iter 450000: 24383.449246694065\n",
      "Loss at iter 451000: 24383.44073771608\n",
      "Loss at iter 452000: 24383.432437074836\n",
      "Loss at iter 453000: 24383.424339669367\n",
      "Loss at iter 454000: 24383.416440523557\n",
      "Loss at iter 455000: 24383.408734783156\n",
      "Loss at iter 456000: 24383.401217712766\n",
      "Loss at iter 457000: 24383.393884692923\n",
      "Loss at iter 458000: 24383.386731217262\n",
      "Loss at iter 459000: 24383.379752889767\n",
      "Loss at iter 460000: 24383.37294542205\n",
      "Loss at iter 461000: 24383.36630463073\n",
      "Loss at iter 462000: 24383.359826434822\n",
      "Loss at iter 463000: 24383.353506853306\n",
      "Loss at iter 464000: 24383.3473420026\n",
      "Loss at iter 465000: 24383.34132809422\n",
      "Loss at iter 466000: 24383.335461432453\n",
      "Loss at iter 467000: 24383.329738412056\n",
      "Loss at iter 468000: 24383.324155516064\n",
      "Loss at iter 469000: 24383.318709313615\n",
      "Loss at iter 470000: 24383.313396457877\n",
      "Loss at iter 471000: 24383.30821368393\n",
      "Loss at iter 472000: 24383.3031578068\n",
      "Loss at iter 473000: 24383.298225719518\n",
      "Loss at iter 474000: 24383.29341439116\n",
      "Loss at iter 475000: 24383.288720865014\n",
      "Loss at iter 476000: 24383.284142256794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 477000: 24383.279675752794\n",
      "Loss at iter 478000: 24383.27531860822\n",
      "Loss at iter 479000: 24383.271068145474\n",
      "Loss at iter 480000: 24383.26692175252\n",
      "Loss at iter 481000: 24383.262876881287\n",
      "Loss at iter 482000: 24383.25893104607\n",
      "Loss at iter 483000: 24383.25508182205\n",
      "Loss at iter 484000: 24383.25132684376\n",
      "Loss at iter 485000: 24383.247663803657\n",
      "Loss at iter 486000: 24383.244090450688\n",
      "Loss at iter 487000: 24383.24060458891\n",
      "Loss at iter 488000: 24383.237204076187\n",
      "Loss at iter 489000: 24383.233886822785\n",
      "Loss at iter 490000: 24383.23065079017\n",
      "Loss at iter 491000: 24383.22749398969\n",
      "Loss at iter 492000: 24383.2244144814\n",
      "Loss at iter 493000: 24383.22141037285\n",
      "Loss at iter 494000: 24383.218479817933\n",
      "Loss at iter 495000: 24383.215621015737\n",
      "Loss at iter 496000: 24383.21283220944\n",
      "Loss at iter 497000: 24383.21011168523\n",
      "Loss at iter 498000: 24383.207457771274\n",
      "Loss at iter 499000: 24383.204868836674\n",
      "Loss at iter 500000: 24383.20234329043\n",
      "Loss at iter 501000: 24383.199879580538\n",
      "Loss at iter 502000: 24383.197476192963\n",
      "Loss at iter 503000: 24383.195131650762\n",
      "Loss at iter 504000: 24383.192844513145\n",
      "Loss at iter 505000: 24383.190613374587\n",
      "Loss at iter 506000: 24383.18843686399\n",
      "Loss at iter 507000: 24383.18631364385\n",
      "Loss at iter 508000: 24383.18424240934\n",
      "Loss at iter 509000: 24383.182221887662\n",
      "Loss at iter 510000: 24383.18025083713\n",
      "Loss at iter 511000: 24383.17832804649\n",
      "Loss at iter 512000: 24383.17645233411\n",
      "Loss at iter 513000: 24383.174622547325\n",
      "Loss at iter 514000: 24383.172837561655\n",
      "Loss at iter 515000: 24383.171096280214\n",
      "Loss at iter 516000: 24383.169397632886\n",
      "Loss at iter 517000: 24383.167740575842\n",
      "Loss at iter 518000: 24383.16612409074\n",
      "Loss at iter 519000: 24383.16454718422\n",
      "Loss at iter 520000: 24383.16300888723\n",
      "Loss at iter 521000: 24383.16150825443\n",
      "Loss at iter 522000: 24383.160044363645\n",
      "Loss at iter 523000: 24383.15861631526\n",
      "Loss at iter 524000: 24383.157223231705\n",
      "Loss at iter 525000: 24383.155864256896\n",
      "Loss at iter 526000: 24383.154538555693\n",
      "Loss at iter 527000: 24383.15324531341\n",
      "Loss at iter 528000: 24383.15198373532\n",
      "Loss at iter 529000: 24383.15075304614\n",
      "Loss at iter 530000: 24383.149552489584\n",
      "Loss at iter 531000: 24383.148381327857\n",
      "Loss at iter 532000: 24383.147238841277\n",
      "Loss at iter 533000: 24383.14612432772\n",
      "Loss at iter 534000: 24383.1450371023\n",
      "Loss at iter 535000: 24383.143976496875\n",
      "Loss at iter 536000: 24383.142941859693\n",
      "Loss at iter 537000: 24383.141932554918\n",
      "Loss at iter 538000: 24383.140947962303\n",
      "Loss at iter 539000: 24383.139987476796\n",
      "Loss at iter 540000: 24383.139050508154\n",
      "Loss at iter 541000: 24383.138136480567\n",
      "Loss at iter 542000: 24383.13724483235\n",
      "Loss at iter 543000: 24383.13637501557\n",
      "Loss at iter 544000: 24383.13552649566\n",
      "Loss at iter 545000: 24383.134698751215\n",
      "Loss at iter 546000: 24383.133891273552\n",
      "Loss at iter 547000: 24383.133103566455\n",
      "Loss at iter 548000: 24383.132335145845\n",
      "Loss at iter 549000: 24383.1315855395\n",
      "Loss at iter 550000: 24383.13085428678\n",
      "Loss at iter 551000: 24383.130140938316\n",
      "Loss at iter 552000: 24383.129445055714\n",
      "Loss at iter 553000: 24383.128766211346\n",
      "Loss at iter 554000: 24383.12810398803\n",
      "Loss at iter 555000: 24383.127457978826\n",
      "Loss at iter 556000: 24383.126827786724\n",
      "Loss at iter 557000: 24383.126213024465\n",
      "Loss at iter 558000: 24383.125613314267\n",
      "Loss at iter 559000: 24383.125028287577\n",
      "Loss at iter 560000: 24383.124457584876\n",
      "Loss at iter 561000: 24383.123900855473\n",
      "Loss at iter 562000: 24383.1233577572\n",
      "Loss at iter 563000: 24383.12282795634\n",
      "Loss at iter 564000: 24383.12231112731\n",
      "Loss at iter 565000: 24383.121806952513\n",
      "Loss at iter 566000: 24383.12131512208\n",
      "Loss at iter 567000: 24383.120835333815\n",
      "Loss at iter 568000: 24383.120367292842\n",
      "Loss at iter 569000: 24383.11991071155\n",
      "Loss at iter 570000: 24383.119465309348\n",
      "Loss at iter 571000: 24383.119030812526\n",
      "Loss at iter 572000: 24383.118606954074\n",
      "Loss at iter 573000: 24383.118193473525\n",
      "Loss at iter 574000: 24383.11779011678\n",
      "Loss at iter 575000: 24383.117396635946\n",
      "Loss at iter 576000: 24383.117012789233\n",
      "Loss at iter 577000: 24383.116638340773\n",
      "Loss at iter 578000: 24383.116273060426\n",
      "Loss at iter 579000: 24383.115916723735\n",
      "Loss at iter 580000: 24383.115569111702\n",
      "Loss at iter 581000: 24383.115230010735\n",
      "Loss at iter 582000: 24383.11489921242\n",
      "Loss at iter 583000: 24383.114576513497\n",
      "Loss at iter 584000: 24383.11426171564\n",
      "Loss at iter 585000: 24383.113954625405\n",
      "Loss at iter 586000: 24383.113655054072\n",
      "Loss at iter 587000: 24383.11336281755\n",
      "Loss at iter 588000: 24383.11307773624\n",
      "Loss at iter 589000: 24383.112799634968\n",
      "Loss at iter 590000: 24383.112528342815\n",
      "Loss at iter 591000: 24383.11226369307\n",
      "Loss at iter 592000: 24383.112005523115\n",
      "Loss at iter 593000: 24383.11175367427\n",
      "Loss at iter 594000: 24383.111507991784\n",
      "Loss at iter 595000: 24383.111268324676\n",
      "Loss at iter 596000: 24383.11103452565\n",
      "Loss at iter 597000: 24383.110806451048\n",
      "Loss at iter 598000: 24383.1105839607\n",
      "Loss at iter 599000: 24383.110366917885\n",
      "Loss at iter 600000: 24383.110155189224\n",
      "Loss at iter 601000: 24383.109948644596\n",
      "Loss at iter 602000: 24383.10974715707\n",
      "Loss at iter 603000: 24383.109550602847\n",
      "Loss at iter 604000: 24383.10935886112\n",
      "Loss at iter 605000: 24383.109171814074\n",
      "Loss at iter 606000: 24383.10898934674\n",
      "Loss at iter 607000: 24383.108811347\n",
      "Loss at iter 608000: 24383.108637705467\n",
      "Loss at iter 609000: 24383.108468315437\n",
      "Loss at iter 610000: 24383.108303072804\n",
      "Loss at iter 611000: 24383.108141876037\n",
      "Loss at iter 612000: 24383.10798462607\n",
      "Loss at iter 613000: 24383.107831226247\n",
      "Loss at iter 614000: 24383.10768158234\n",
      "Loss at iter 615000: 24383.107535602358\n",
      "Loss at iter 616000: 24383.1073931966\n",
      "Loss at iter 617000: 24383.107254277547\n",
      "Loss at iter 618000: 24383.107118759835\n",
      "Loss at iter 619000: 24383.106986560208\n",
      "Loss at iter 620000: 24383.10685759738\n",
      "Loss at iter 621000: 24383.10673179213\n",
      "Loss at iter 622000: 24383.106609067137\n",
      "Loss at iter 623000: 24383.10648934698\n",
      "Loss at iter 624000: 24383.1063725581\n",
      "Loss at iter 625000: 24383.10625862872\n",
      "Loss at iter 626000: 24383.106147488834\n",
      "Loss at iter 627000: 24383.106039070113\n",
      "Loss at iter 628000: 24383.10593330597\n",
      "Loss at iter 629000: 24383.105830131393\n",
      "Loss at iter 630000: 24383.10572948297\n",
      "Loss at iter 631000: 24383.105631298862\n",
      "Loss at iter 632000: 24383.10553551873\n",
      "Loss at iter 633000: 24383.105442083706\n",
      "Loss at iter 634000: 24383.105350936374\n",
      "Loss at iter 635000: 24383.105262020734\n",
      "Loss at iter 636000: 24383.105175282126\n",
      "Loss at iter 637000: 24383.105090667264\n",
      "Loss at iter 638000: 24383.105008124145\n",
      "Loss at iter 639000: 24383.104927602028\n",
      "Loss at iter 640000: 24383.104849051444\n",
      "Loss at iter 641000: 24383.104772424133\n",
      "Loss at iter 642000: 24383.104697672978\n",
      "Loss at iter 643000: 24383.104624752057\n",
      "Loss at iter 644000: 24383.10455361656\n",
      "Loss at iter 645000: 24383.10448422278\n",
      "Loss at iter 646000: 24383.104416528044\n",
      "Loss at iter 647000: 24383.10435049078\n",
      "Loss at iter 648000: 24383.10428607039\n",
      "Loss at iter 649000: 24383.10422322729\n",
      "Loss at iter 650000: 24383.10416192287\n",
      "Loss at iter 651000: 24383.104102119443\n",
      "Loss at iter 652000: 24383.104043780262\n",
      "Loss at iter 653000: 24383.10398686948\n",
      "Loss at iter 654000: 24383.103931352125\n",
      "Loss at iter 655000: 24383.103877194066\n",
      "Loss at iter 656000: 24383.103824362042\n",
      "Loss at iter 657000: 24383.10377282357\n",
      "Loss at iter 658000: 24383.103722546984\n",
      "Loss at iter 659000: 24383.10367350138\n",
      "Loss at iter 660000: 24383.10362565664\n",
      "Loss at iter 661000: 24383.10357898334\n",
      "Loss at iter 662000: 24383.103533452802\n",
      "Loss at iter 663000: 24383.10348903706\n",
      "Loss at iter 664000: 24383.103445708803\n",
      "Loss at iter 665000: 24383.10340344141\n",
      "Loss at iter 666000: 24383.103362208894\n",
      "Loss at iter 667000: 24383.103321985942\n",
      "Loss at iter 668000: 24383.103282747823\n",
      "Loss at iter 669000: 24383.103244470425\n",
      "Loss at iter 670000: 24383.103207130214\n",
      "Loss at iter 671000: 24383.10317070427\n",
      "Loss at iter 672000: 24383.10313517017\n",
      "Loss at iter 673000: 24383.103100506112\n",
      "Loss at iter 674000: 24383.10306669078\n",
      "Loss at iter 675000: 24383.103033703395\n",
      "Loss at iter 676000: 24383.103001523687\n",
      "Loss at iter 677000: 24383.102970131873\n",
      "Loss at iter 678000: 24383.102939508666\n",
      "Loss at iter 679000: 24383.10290963525\n",
      "Loss at iter 680000: 24383.102880493258\n",
      "Loss at iter 681000: 24383.102852064796\n",
      "Loss at iter 682000: 24383.10282433239\n",
      "Loss at iter 683000: 24383.102797278993\n",
      "Loss at iter 684000: 24383.102770887985\n",
      "Loss at iter 685000: 24383.102745143136\n",
      "Loss at iter 686000: 24383.102720028626\n",
      "Loss at iter 687000: 24383.102695529036\n",
      "Loss at iter 688000: 24383.102671629294\n",
      "Loss at iter 689000: 24383.102648314733\n",
      "Loss at iter 690000: 24383.102625571013\n",
      "Loss at iter 691000: 24383.10260338416\n",
      "Loss at iter 692000: 24383.102581740528\n",
      "Loss at iter 693000: 24383.102560626827\n",
      "Loss at iter 694000: 24383.10254003009\n",
      "Loss at iter 695000: 24383.10251993764\n",
      "Loss at iter 696000: 24383.102500337154\n",
      "Loss at iter 697000: 24383.102481216563\n",
      "Loss at iter 698000: 24383.10246256413\n",
      "Loss at iter 699000: 24383.102444368385\n",
      "Loss at iter 700000: 24383.102426618156\n",
      "Loss at iter 701000: 24383.102409302526\n",
      "Loss at iter 702000: 24383.10239241086\n",
      "Loss at iter 703000: 24383.102375932776\n",
      "Loss at iter 704000: 24383.102359858152\n",
      "Loss at iter 705000: 24383.102344177092\n",
      "Loss at iter 706000: 24383.102328879988\n",
      "Loss at iter 707000: 24383.102313957414\n",
      "Loss at iter 708000: 24383.102299400205\n",
      "Loss at iter 709000: 24383.10228519943\n",
      "Loss at iter 710000: 24383.102271346343\n",
      "Loss at iter 711000: 24383.102257832445\n",
      "Loss at iter 712000: 24383.102244649417\n",
      "Loss at iter 713000: 24383.102231789187\n",
      "Loss at iter 714000: 24383.10221924381\n",
      "Loss at iter 715000: 24383.10220700561\n",
      "Loss at iter 716000: 24383.102195067055\n",
      "Loss at iter 717000: 24383.102183420804\n",
      "Loss at iter 718000: 24383.1021720597\n",
      "Loss at iter 719000: 24383.10216097677\n",
      "Loss at iter 720000: 24383.102150165196\n",
      "Loss at iter 721000: 24383.102139618342\n",
      "Loss at iter 722000: 24383.10212932972\n",
      "Loss at iter 723000: 24383.102119293\n",
      "Loss at iter 724000: 24383.10210950203\n",
      "Loss at iter 725000: 24383.102099950782\n",
      "Loss at iter 726000: 24383.102090633387\n",
      "Loss at iter 727000: 24383.10208154413\n",
      "Loss at iter 728000: 24383.102072677415\n",
      "Loss at iter 729000: 24383.10206402779\n",
      "Loss at iter 730000: 24383.102055589956\n",
      "Loss at iter 731000: 24383.102047358712\n",
      "Loss at iter 732000: 24383.102039329\n",
      "Loss at iter 733000: 24383.1020314959\n",
      "Loss at iter 734000: 24383.102023854575\n",
      "Loss at iter 735000: 24383.10201640036\n",
      "Loss at iter 736000: 24383.10200912864\n",
      "Loss at iter 737000: 24383.102002034968\n",
      "Loss at iter 738000: 24383.101995114983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 739000: 24383.10198836443\n",
      "Loss at iter 740000: 24383.101981779153\n",
      "Loss at iter 741000: 24383.101975355123\n",
      "Loss at iter 742000: 24383.101969088373\n",
      "Loss at iter 743000: 24383.101962975066\n",
      "Loss at iter 744000: 24383.10195701143\n",
      "Loss at iter 745000: 24383.10195119382\n",
      "Loss at iter 746000: 24383.101945518636\n",
      "Loss at iter 747000: 24383.101939982414\n",
      "Loss at iter 748000: 24383.101934581748\n",
      "Loss at iter 749000: 24383.10192931331\n",
      "Loss at iter 750000: 24383.101924173872\n",
      "Loss at iter 751000: 24383.10191916026\n",
      "Loss at iter 752000: 24383.101914269402\n",
      "Loss at iter 753000: 24383.101909498295\n",
      "Loss at iter 754000: 24383.101904844014\n",
      "Loss at iter 755000: 24383.101900303685\n",
      "Loss at iter 756000: 24383.10189587452\n",
      "Loss at iter 757000: 24383.1018915538\n",
      "Loss at iter 758000: 24383.10188733888\n",
      "Loss at iter 759000: 24383.101883227144\n",
      "Loss at iter 760000: 24383.101879216098\n",
      "Loss at iter 761000: 24383.101875303244\n",
      "Loss at iter 762000: 24383.101871486204\n",
      "Loss at iter 763000: 24383.101867762616\n",
      "Loss at iter 764000: 24383.1018641302\n",
      "Loss at iter 765000: 24383.10186058672\n",
      "Loss at iter 766000: 24383.10185713001\n",
      "Loss at iter 767000: 24383.101853757926\n",
      "Loss at iter 768000: 24383.101850468396\n",
      "Loss at iter 769000: 24383.101847259422\n",
      "Loss at iter 770000: 24383.101844129014\n",
      "Loss at iter 771000: 24383.101841075255\n",
      "Loss at iter 772000: 24383.101838096245\n",
      "Loss at iter 773000: 24383.10183519021\n",
      "Loss at iter 774000: 24383.101832355303\n",
      "Loss at iter 775000: 24383.10182958981\n",
      "Loss at iter 776000: 24383.10182689203\n",
      "Loss at iter 777000: 24383.1018242603\n",
      "Loss at iter 778000: 24383.101821693017\n",
      "Loss at iter 779000: 24383.101819188585\n",
      "Loss at iter 780000: 24383.10181674547\n",
      "Loss at iter 781000: 24383.101814362177\n",
      "Loss at iter 782000: 24383.10181203723\n",
      "Loss at iter 783000: 24383.101809769214\n",
      "Loss at iter 784000: 24383.10180755673\n",
      "Loss at iter 785000: 24383.10180539842\n",
      "Loss at iter 786000: 24383.10180329295\n",
      "Loss at iter 787000: 24383.101801239038\n",
      "Loss at iter 788000: 24383.101799235403\n",
      "Loss at iter 789000: 24383.10179728083\n",
      "Loss at iter 790000: 24383.101795374114\n",
      "Loss at iter 791000: 24383.101793514084\n",
      "Loss at iter 792000: 24383.101791699595\n",
      "Loss at iter 793000: 24383.10178992954\n",
      "Loss at iter 794000: 24383.101788202806\n",
      "Loss at iter 795000: 24383.101786518364\n",
      "Loss at iter 796000: 24383.101784875158\n",
      "Loss at iter 797000: 24383.10178327219\n",
      "Loss at iter 798000: 24383.10178170847\n",
      "Loss at iter 799000: 24383.10178018303\n",
      "Loss at iter 800000: 24383.101778694938\n",
      "Loss at iter 801000: 24383.101777243293\n",
      "Loss at iter 802000: 24383.101775827174\n",
      "Loss at iter 803000: 24383.101774445746\n",
      "Loss at iter 804000: 24383.10177309813\n",
      "Loss at iter 805000: 24383.101771783513\n",
      "Loss at iter 806000: 24383.101770501085\n",
      "Loss at iter 807000: 24383.10176925005\n",
      "Loss at iter 808000: 24383.10176802965\n",
      "Loss at iter 809000: 24383.101766839132\n",
      "Loss at iter 810000: 24383.101765677762\n",
      "Loss at iter 811000: 24383.101764544823\n",
      "Loss at iter 812000: 24383.101763439638\n",
      "Loss at iter 813000: 24383.10176236149\n",
      "Loss at iter 814000: 24383.101761309757\n",
      "Loss at iter 815000: 24383.10176028376\n",
      "Loss at iter 816000: 24383.101759282898\n",
      "Loss at iter 817000: 24383.101758306537\n",
      "Loss at iter 818000: 24383.10175735408\n",
      "Loss at iter 819000: 24383.101756424952\n",
      "Loss at iter 820000: 24383.10175551856\n",
      "Loss at iter 821000: 24383.10175463437\n",
      "Loss at iter 822000: 24383.101753771825\n",
      "Loss at iter 823000: 24383.101752930397\n",
      "Loss at iter 824000: 24383.10175210957\n",
      "Loss at iter 825000: 24383.101751308845\n",
      "Loss at iter 826000: 24383.101750527734\n",
      "Loss at iter 827000: 24383.10174976573\n",
      "Loss at iter 828000: 24383.101749022397\n",
      "Loss at iter 829000: 24383.101748297257\n",
      "Loss at iter 830000: 24383.101747589866\n",
      "Loss at iter 831000: 24383.10174689981\n",
      "Loss at iter 832000: 24383.101746226635\n",
      "Loss at iter 833000: 24383.10174556995\n",
      "Loss at iter 834000: 24383.101744929336\n",
      "Loss at iter 835000: 24383.10174430441\n",
      "Loss at iter 836000: 24383.101743694788\n",
      "Loss at iter 837000: 24383.101743100102\n",
      "Loss at iter 838000: 24383.101742519957\n",
      "Loss at iter 839000: 24383.10174195403\n",
      "Loss at iter 840000: 24383.101741401955\n",
      "Loss at iter 841000: 24383.10174086339\n",
      "Loss at iter 842000: 24383.101740338017\n",
      "Loss at iter 843000: 24383.10173982552\n",
      "Loss at iter 844000: 24383.101739325557\n",
      "Loss at iter 845000: 24383.101738837835\n",
      "Loss at iter 846000: 24383.101738362056\n",
      "Loss at iter 847000: 24383.10173789793\n",
      "Loss at iter 848000: 24383.10173744517\n",
      "Loss at iter 849000: 24383.10173700349\n",
      "Loss at iter 850000: 24383.101736572622\n",
      "Loss at iter 851000: 24383.101736152308\n",
      "Loss at iter 852000: 24383.101735742286\n",
      "Loss at iter 853000: 24383.1017353423\n",
      "Loss at iter 854000: 24383.10173495211\n",
      "Loss at iter 855000: 24383.10173457147\n",
      "Loss at iter 856000: 24383.10173420015\n",
      "Loss at iter 857000: 24383.101733837924\n",
      "Loss at iter 858000: 24383.101733484575\n",
      "Loss at iter 859000: 24383.10173313986\n",
      "Loss at iter 860000: 24383.101732803596\n",
      "Loss at iter 861000: 24383.101732475567\n",
      "Loss at iter 862000: 24383.101732155574\n",
      "Loss at iter 863000: 24383.101731843402\n",
      "Loss at iter 864000: 24383.101731538874\n",
      "Loss at iter 865000: 24383.101731241808\n",
      "Loss at iter 866000: 24383.10173095202\n",
      "Loss at iter 867000: 24383.10173066932\n",
      "Loss at iter 868000: 24383.101730393544\n",
      "Loss at iter 869000: 24383.10173012452\n",
      "Loss at iter 870000: 24383.101729862083\n",
      "Loss at iter 871000: 24383.10172960607\n",
      "Loss at iter 872000: 24383.101729356327\n",
      "Loss at iter 873000: 24383.1017291127\n",
      "Loss at iter 874000: 24383.101728875037\n",
      "Loss at iter 875000: 24383.10172864319\n",
      "Loss at iter 876000: 24383.10172841702\n",
      "Loss at iter 877000: 24383.101728196387\n",
      "Loss at iter 878000: 24383.101727981168\n",
      "Loss at iter 879000: 24383.10172777121\n",
      "Loss at iter 880000: 24383.101727566383\n",
      "Loss at iter 881000: 24383.101727366582\n",
      "Loss at iter 882000: 24383.10172717167\n",
      "Loss at iter 883000: 24383.10172698153\n",
      "Loss at iter 884000: 24383.10172679605\n",
      "Loss at iter 885000: 24383.101726615107\n",
      "Loss at iter 886000: 24383.1017264386\n",
      "Loss at iter 887000: 24383.101726266406\n",
      "Loss at iter 888000: 24383.101726098434\n",
      "Loss at iter 889000: 24383.10172593457\n",
      "Loss at iter 890000: 24383.101725774723\n",
      "Loss at iter 891000: 24383.10172561879\n",
      "Loss at iter 892000: 24383.101725466673\n",
      "Loss at iter 893000: 24383.101725318284\n",
      "Loss at iter 894000: 24383.101725173514\n",
      "Loss at iter 895000: 24383.101725032302\n",
      "Loss at iter 896000: 24383.101724894543\n",
      "Loss at iter 897000: 24383.101724760163\n",
      "Loss at iter 898000: 24383.10172462907\n",
      "Loss at iter 899000: 24383.10172450118\n",
      "Loss at iter 900000: 24383.10172437643\n",
      "Loss at iter 901000: 24383.10172425473\n",
      "Loss at iter 902000: 24383.10172413601\n",
      "Loss at iter 903000: 24383.1017240202\n",
      "Loss at iter 904000: 24383.101723907213\n",
      "Loss at iter 905000: 24383.101723797005\n",
      "Loss at iter 906000: 24383.101723689502\n",
      "Loss at iter 907000: 24383.101723584616\n",
      "Loss at iter 908000: 24383.1017234823\n",
      "Loss at iter 909000: 24383.101723382497\n",
      "Loss at iter 910000: 24383.101723285134\n",
      "Loss at iter 911000: 24383.101723190157\n",
      "Loss at iter 912000: 24383.1017230975\n",
      "Loss at iter 913000: 24383.101723007112\n",
      "Loss at iter 914000: 24383.101722918942\n",
      "Loss at iter 915000: 24383.101722832937\n",
      "Loss at iter 916000: 24383.101722749023\n",
      "Loss at iter 917000: 24383.101722667176\n",
      "Loss at iter 918000: 24383.101722587322\n",
      "Loss at iter 919000: 24383.101722509426\n",
      "Loss at iter 920000: 24383.101722433435\n",
      "Loss at iter 921000: 24383.101722359315\n",
      "Loss at iter 922000: 24383.101722287\n",
      "Loss at iter 923000: 24383.101722216463\n",
      "Loss at iter 924000: 24383.10172214765\n",
      "Loss at iter 925000: 24383.101722080522\n",
      "Loss at iter 926000: 24383.10172201503\n",
      "Loss at iter 927000: 24383.101721951156\n",
      "Loss at iter 928000: 24383.101721888834\n",
      "Loss at iter 929000: 24383.10172182804\n",
      "Loss at iter 930000: 24383.10172176874\n",
      "Loss at iter 931000: 24383.101721710893\n",
      "Loss at iter 932000: 24383.10172165446\n",
      "Loss at iter 933000: 24383.101721599407\n",
      "Loss at iter 934000: 24383.101721545696\n",
      "Loss at iter 935000: 24383.10172149331\n",
      "Loss at iter 936000: 24383.101721442203\n",
      "Loss at iter 937000: 24383.101721392337\n",
      "Loss at iter 938000: 24383.101721343704\n",
      "Loss at iter 939000: 24383.101721296258\n",
      "Loss at iter 940000: 24383.101721249976\n",
      "Loss at iter 941000: 24383.10172120482\n",
      "Loss at iter 942000: 24383.101721160787\n",
      "Loss at iter 943000: 24383.10172111782\n",
      "Loss at iter 944000: 24383.1017210759\n",
      "Loss at iter 945000: 24383.101721035015\n",
      "Loss at iter 946000: 24383.101720995135\n",
      "Loss at iter 947000: 24383.10172095622\n",
      "Loss at iter 948000: 24383.10172091826\n",
      "Loss at iter 949000: 24383.101720881234\n",
      "Loss at iter 950000: 24383.10172084512\n",
      "Loss at iter 951000: 24383.10172080988\n",
      "Loss at iter 952000: 24383.101720775503\n",
      "Loss at iter 953000: 24383.101720741975\n",
      "Loss at iter 954000: 24383.101720709266\n",
      "Loss at iter 955000: 24383.101720677354\n",
      "Loss at iter 956000: 24383.101720646213\n",
      "Loss at iter 957000: 24383.101720615854\n",
      "Loss at iter 958000: 24383.10172058623\n",
      "Loss at iter 959000: 24383.10172055733\n",
      "Loss at iter 960000: 24383.101720529135\n",
      "Loss at iter 961000: 24383.101720501636\n",
      "Loss at iter 962000: 24383.101720474802\n",
      "Loss at iter 963000: 24383.10172044864\n",
      "Loss at iter 964000: 24383.101720423107\n",
      "Loss at iter 965000: 24383.101720398205\n",
      "Loss at iter 966000: 24383.101720373914\n",
      "Loss at iter 967000: 24383.10172035021\n",
      "Loss at iter 968000: 24383.101720327093\n",
      "Loss at iter 969000: 24383.101720304534\n",
      "Loss at iter 970000: 24383.10172028253\n",
      "Loss at iter 971000: 24383.101720261075\n",
      "Loss at iter 972000: 24383.10172024013\n",
      "Loss at iter 973000: 24383.101720219707\n",
      "Loss at iter 974000: 24383.101720199786\n",
      "Loss at iter 975000: 24383.101720180348\n",
      "Loss at iter 976000: 24383.101720161387\n",
      "Loss at iter 977000: 24383.1017201429\n",
      "Loss at iter 978000: 24383.101720124847\n",
      "Loss at iter 979000: 24383.101720107243\n",
      "Loss at iter 980000: 24383.101720090082\n",
      "Loss at iter 981000: 24383.101720073326\n",
      "Loss at iter 982000: 24383.101720056984\n",
      "Loss at iter 983000: 24383.101720041046\n",
      "Loss at iter 984000: 24383.101720025494\n",
      "Loss at iter 985000: 24383.101720010327\n",
      "Loss at iter 986000: 24383.10171999553\n",
      "Loss at iter 987000: 24383.101719981092\n",
      "Loss at iter 988000: 24383.101719967013\n",
      "Loss at iter 989000: 24383.101719953273\n",
      "Loss at iter 990000: 24383.101719939874\n",
      "Loss at iter 991000: 24383.1017199268\n",
      "Loss at iter 992000: 24383.101719914044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 993000: 24383.101719901602\n",
      "Loss at iter 994000: 24383.10171988947\n",
      "Loss at iter 995000: 24383.10171987763\n",
      "Loss at iter 996000: 24383.10171986608\n",
      "Loss at iter 997000: 24383.10171985482\n",
      "Loss at iter 998000: 24383.10171984383\n",
      "Loss at iter 999000: 24383.101719833103\n",
      "Final loss:  24383.10171982266\n",
      "Final W:  [[-415.47940439]\n",
      " [   1.63051693]\n",
      " [ 181.33377584]]\n"
     ]
    }
   ],
   "source": [
    "#Khởi tạo lr\n",
    "np.random.seed(1)\n",
    "lr= 0.0001  \n",
    "#Khởi tạo W\n",
    "W=np.asarray([[1.0], [2.0],[3.0]])\n",
    "#Thêm một cột cho bias\n",
    "X=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1) \n",
    "print(\"shape của X là:\", X.shape)\n",
    "print()\n",
    "#GD\n",
    "for i in range(1000000):#gồm 10 vòng lặp\n",
    "    prediction=np.matmul(X,W)\n",
    "    loss=prediction-y_train\n",
    "    gradient=np.matmul(X.T,loss)\n",
    "    W=W-lr*(1/X.shape[0])*gradient\n",
    "    if i % 1000 == 0:\n",
    "        loss = np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "        print(\"Loss at iter {}: {}\".format(i, loss))\n",
    "loss=np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "print(\"Final loss: \", loss)\n",
    "print(\"Final W: \", W)\n",
    "saveW.append(W)\n",
    "saveloss.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape của X là: (127, 3)\n",
      "\n",
      "Loss at iter 0: 67786.09937069476\n",
      "Loss at iter 1000: 25484.936169656787\n",
      "Loss at iter 2000: 24460.840407075877\n",
      "Loss at iter 3000: 24427.328542197934\n",
      "Loss at iter 4000: 24415.3178428021\n",
      "Loss at iter 5000: 24388.330084261856\n",
      "Loss at iter 6000: 24384.458725810535\n",
      "Loss at iter 7000: 24393.046388931998\n",
      "Loss at iter 8000: 24432.02185035146\n",
      "Loss at iter 9000: 24403.948384189145\n",
      "Loss:  24411.114319291002\n",
      "W:  [[-415.93979903]\n",
      " [   1.2317513 ]\n",
      " [ 181.14588101]]\n"
     ]
    }
   ],
   "source": [
    "lr=0.0001\n",
    "#Khởi tạo W\n",
    "W=np.asarray([[1.0], [2.0],[3.0]])\n",
    "#Thêm một cột cho bias\n",
    "X=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1) \n",
    "print(\"shape của X là:\", X.shape)\n",
    "print()\n",
    "for i in range(10000):#gồm 100000 vòng lặp\n",
    "    rd_id = np.random.permutation(X.shape[0])\n",
    "    for j in rd_id:\n",
    "        X_1=X[j,:].reshape(1,-1)\n",
    "        error=np.matmul(X_1,W)-y_train[j,:].reshape(1,-1)\n",
    "        W=W-lr*np.matmul(X_1.T,error)\n",
    "    if i % 1000 == 0:\n",
    "        loss = np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "        print(\"Loss at iter {}: {}\".format(i, loss))\n",
    "loss=np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "print(\"Loss: \", loss)\n",
    "print(\"W: \", W)\n",
    "saveW.append(W)\n",
    "saveloss.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape của X là: (127, 3)\n",
      "\n",
      "1\n",
      "Loss at iter 0: 213230.19957605074\n",
      "1\n",
      "Loss at iter 1000: 44043.69256852763\n",
      "1\n",
      "Loss at iter 2000: 39228.42353051554\n",
      "1\n",
      "Loss at iter 3000: 35934.11870221792\n",
      "1\n",
      "Loss at iter 4000: 33395.486075491455\n",
      "1\n",
      "Loss at iter 5000: 31416.507526981037\n",
      "1\n",
      "Loss at iter 6000: 29872.20205082483\n",
      "1\n",
      "Loss at iter 7000: 28666.983858921714\n",
      "1\n",
      "Loss at iter 8000: 27726.390836765924\n",
      "1\n",
      "Loss at iter 9000: 26992.319698195875\n",
      "1\n",
      "Loss at iter 10000: 26419.42530118362\n",
      "1\n",
      "Loss at iter 11000: 25972.31878134973\n",
      "1\n",
      "Loss at iter 12000: 25623.38145323721\n",
      "1\n",
      "Loss at iter 13000: 25351.058743648173\n",
      "1\n",
      "Loss at iter 14000: 25138.528721253093\n",
      "1\n",
      "Loss at iter 15000: 24972.662971432135\n",
      "1\n",
      "Loss at iter 16000: 24843.21562722336\n",
      "1\n",
      "Loss at iter 17000: 24742.190461143015\n",
      "1\n",
      "Loss at iter 18000: 24663.346941842206\n",
      "1\n",
      "Loss at iter 19000: 24601.814743708608\n",
      "1\n",
      "Loss at iter 20000: 24553.792896881758\n",
      "1\n",
      "Loss at iter 21000: 24516.314993562675\n",
      "1\n",
      "Loss at iter 22000: 24487.06594693066\n",
      "1\n",
      "Loss at iter 23000: 24464.238983490417\n",
      "1\n",
      "Loss at iter 24000: 24446.42403497403\n",
      "1\n",
      "Loss at iter 25000: 24432.52063553713\n",
      "1\n",
      "Loss at iter 26000: 24421.669943730507\n",
      "1\n",
      "Loss at iter 27000: 24413.201690105467\n",
      "1\n",
      "Loss at iter 28000: 24406.59277329754\n",
      "1\n",
      "Loss at iter 29000: 24401.434946984366\n",
      "1\n",
      "Loss at iter 30000: 24397.409601674368\n",
      "1\n",
      "Loss at iter 31000: 24394.26808354621\n",
      "1\n",
      "Loss at iter 32000: 24391.816334593135\n",
      "1\n",
      "Loss at iter 33000: 24389.90290526259\n",
      "1\n",
      "Loss at iter 34000: 24388.409599107228\n",
      "1\n",
      "Loss at iter 35000: 24387.244171548427\n",
      "1\n",
      "Loss at iter 36000: 24386.334631739774\n",
      "1\n",
      "Loss at iter 37000: 24385.624795545096\n",
      "1\n",
      "Loss at iter 38000: 24385.07081492929\n",
      "1\n",
      "Loss at iter 39000: 24384.638469375517\n",
      "1\n",
      "Loss at iter 40000: 24384.301052013725\n",
      "1\n",
      "Loss at iter 41000: 24384.037719882643\n",
      "1\n",
      "Loss at iter 42000: 24383.832206417374\n",
      "1\n",
      "Loss at iter 43000: 24383.671816630427\n",
      "1\n",
      "Loss at iter 44000: 24383.546642916255\n",
      "1\n",
      "Loss at iter 45000: 24383.44895303825\n",
      "1\n",
      "Loss at iter 46000: 24383.372712492644\n",
      "1\n",
      "Loss at iter 47000: 24383.313211744848\n",
      "1\n",
      "Loss at iter 48000: 24383.26677531182\n",
      "1\n",
      "Loss at iter 49000: 24383.230534720045\n",
      "1\n",
      "Loss at iter 50000: 24383.202251313964\n",
      "1\n",
      "Loss at iter 51000: 24383.18017796967\n",
      "1\n",
      "Loss at iter 52000: 24383.162951171485\n",
      "1\n",
      "Loss at iter 53000: 24383.149506784826\n",
      "1\n",
      "Loss at iter 54000: 24383.139014322405\n",
      "1\n",
      "Loss at iter 55000: 24383.13082564333\n",
      "1\n",
      "Loss at iter 56000: 24383.12443491613\n",
      "1\n",
      "Loss at iter 57000: 24383.1194473725\n",
      "1\n",
      "Loss at iter 58000: 24383.115554921667\n",
      "1\n",
      "Loss at iter 59000: 24383.112517118967\n",
      "1\n",
      "Loss at iter 60000: 24383.110146313098\n",
      "1\n",
      "Loss at iter 61000: 24383.10829605454\n",
      "1\n",
      "Loss at iter 62000: 24383.10685204904\n",
      "1\n",
      "Loss at iter 63000: 24383.105725097397\n",
      "1\n",
      "Loss at iter 64000: 24383.10484558553\n",
      "1\n",
      "Loss at iter 65000: 24383.104159184168\n",
      "1\n",
      "Loss at iter 66000: 24383.103623492916\n",
      "1\n",
      "Loss at iter 67000: 24383.10320542099\n",
      "1\n",
      "Loss at iter 68000: 24383.10287914327\n",
      "1\n",
      "Loss at iter 69000: 24383.102624504903\n",
      "1\n",
      "Loss at iter 70000: 24383.102425776346\n",
      "1\n",
      "Loss at iter 71000: 24383.10227068174\n",
      "1\n",
      "Loss at iter 72000: 24383.102149640566\n",
      "1\n",
      "Loss at iter 73000: 24383.102055175863\n",
      "1\n",
      "Loss at iter 74000: 24383.101981452357\n",
      "1\n",
      "Loss at iter 75000: 24383.10192391598\n",
      "1\n",
      "Loss at iter 76000: 24383.10187901262\n",
      "1\n",
      "Loss at iter 77000: 24383.101843968496\n",
      "1\n",
      "Loss at iter 78000: 24383.101816618855\n",
      "1\n",
      "Loss at iter 79000: 24383.101795274244\n",
      "1\n",
      "Loss at iter 80000: 24383.101778616186\n",
      "1\n",
      "Loss at iter 81000: 24383.101765615662\n",
      "1\n",
      "Loss at iter 82000: 24383.10175546959\n",
      "1\n",
      "Loss at iter 83000: 24383.10174755126\n",
      "1\n",
      "Loss at iter 84000: 24383.101741371516\n",
      "1\n",
      "Loss at iter 85000: 24383.101736548633\n",
      "1\n",
      "Loss at iter 86000: 24383.10173278469\n",
      "1\n",
      "Loss at iter 87000: 24383.10172984718\n",
      "1\n",
      "Loss at iter 88000: 24383.101727554647\n",
      "1\n",
      "Loss at iter 89000: 24383.101725765468\n",
      "1\n",
      "Loss at iter 90000: 24383.101724369142\n",
      "1\n",
      "Loss at iter 91000: 24383.10172327939\n",
      "1\n",
      "Loss at iter 92000: 24383.10172242892\n",
      "1\n",
      "Loss at iter 93000: 24383.101721765175\n",
      "1\n",
      "Loss at iter 94000: 24383.101721247167\n",
      "1\n",
      "Loss at iter 95000: 24383.101720842904\n",
      "1\n",
      "Loss at iter 96000: 24383.10172052739\n",
      "1\n",
      "Loss at iter 97000: 24383.101720281164\n",
      "1\n",
      "Loss at iter 98000: 24383.101720088995\n",
      "1\n",
      "Loss at iter 99000: 24383.101719939023\n",
      "1\n",
      "Loss at iter 100000: 24383.101719821974\n",
      "1\n",
      "Loss at iter 101000: 24383.101719730632\n",
      "1\n",
      "Loss at iter 102000: 24383.101719659342\n",
      "1\n",
      "Loss at iter 103000: 24383.101719603706\n",
      "1\n",
      "Loss at iter 104000: 24383.101719560284\n",
      "1\n",
      "Loss at iter 105000: 24383.101719526403\n",
      "1\n",
      "Loss at iter 106000: 24383.101719499948\n",
      "1\n",
      "Loss at iter 107000: 24383.101719479313\n",
      "1\n",
      "Loss at iter 108000: 24383.101719463204\n",
      "1\n",
      "Loss at iter 109000: 24383.101719450624\n",
      "1\n",
      "Loss at iter 110000: 24383.101719440823\n",
      "1\n",
      "Loss at iter 111000: 24383.101719433165\n",
      "1\n",
      "Loss at iter 112000: 24383.101719427184\n",
      "1\n",
      "Loss at iter 113000: 24383.10171942253\n",
      "1\n",
      "Loss at iter 114000: 24383.101719418883\n",
      "1\n",
      "Loss at iter 115000: 24383.101719416045\n",
      "1\n",
      "Loss at iter 116000: 24383.101719413826\n",
      "1\n",
      "Loss at iter 117000: 24383.101719412094\n",
      "1\n",
      "Loss at iter 118000: 24383.101719410744\n",
      "1\n",
      "Loss at iter 119000: 24383.10171940969\n",
      "1\n",
      "Loss at iter 120000: 24383.101719408867\n",
      "1\n",
      "Loss at iter 121000: 24383.101719408238\n",
      "1\n",
      "Loss at iter 122000: 24383.10171940773\n",
      "1\n",
      "Loss at iter 123000: 24383.101719407336\n",
      "1\n",
      "Loss at iter 124000: 24383.101719407026\n",
      "1\n",
      "Loss at iter 125000: 24383.10171940679\n",
      "1\n",
      "Loss at iter 126000: 24383.101719406604\n",
      "1\n",
      "Loss at iter 127000: 24383.101719406463\n",
      "1\n",
      "Loss at iter 128000: 24383.101719406353\n",
      "1\n",
      "Loss at iter 129000: 24383.101719406262\n",
      "1\n",
      "Loss at iter 130000: 24383.101719406186\n",
      "1\n",
      "Loss at iter 131000: 24383.10171940613\n",
      "1\n",
      "Loss at iter 132000: 24383.101719406095\n",
      "1\n",
      "Loss at iter 133000: 24383.10171940607\n",
      "1\n",
      "Loss at iter 134000: 24383.10171940604\n",
      "1\n",
      "Loss at iter 135000: 24383.101719406015\n",
      "1\n",
      "Loss at iter 136000: 24383.101719405997\n",
      "1\n",
      "Loss at iter 137000: 24383.101719405993\n",
      "1\n",
      "Loss at iter 138000: 24383.101719405986\n",
      "1\n",
      "Loss at iter 139000: 24383.10171940597\n",
      "1\n",
      "Loss at iter 140000: 24383.10171940597\n",
      "1\n",
      "Loss at iter 141000: 24383.10171940596\n",
      "1\n",
      "Loss at iter 142000: 24383.101719405968\n",
      "1\n",
      "Loss at iter 143000: 24383.10171940596\n",
      "1\n",
      "Loss at iter 144000: 24383.10171940596\n",
      "1\n",
      "Loss at iter 145000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 146000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 147000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 148000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 149000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 150000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 151000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 152000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 153000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 154000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 155000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 156000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 157000: 24383.101719405942\n",
      "1\n",
      "Loss at iter 158000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 159000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 160000: 24383.101719405942\n",
      "1\n",
      "Loss at iter 161000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 162000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 163000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 164000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 165000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 166000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 167000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 168000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 169000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 170000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 171000: 24383.101719405942\n",
      "1\n",
      "Loss at iter 172000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 173000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 174000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 175000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 176000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 177000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 178000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 179000: 24383.101719405953\n",
      "1\n",
      "Loss at iter 180000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 181000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 182000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 183000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 184000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 185000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 186000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 187000: 24383.101719405942\n",
      "1\n",
      "Loss at iter 188000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 189000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 190000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 191000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 192000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 193000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 194000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 195000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 196000: 24383.101719405946\n",
      "1\n",
      "Loss at iter 197000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 198000: 24383.10171940595\n",
      "1\n",
      "Loss at iter 199000: 24383.101719405942\n",
      "Loss:  24383.10171940595\n",
      "W:  [[-415.48120459]\n",
      " [   1.63051922]\n",
      " [ 181.33412414]]\n"
     ]
    }
   ],
   "source": [
    "#Khởi tạo W\n",
    "lr=0.001\n",
    "W=np.asarray([[1.0], [2.0],[3.0]])\n",
    "#Thêm một cột cho bias\n",
    "X=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1) \n",
    "first_X=X.copy()\n",
    "first_y=y_train.copy()\n",
    "print(\"shape của X là:\", X.shape)\n",
    "print()\n",
    "for i in range(200000):#gồm 100000 vòng lặp\n",
    "  indices = np.random.permutation(X.shape[0])\n",
    "  first_X=first_X[indices]\n",
    "  first_y=first_y[indices]\n",
    "\n",
    "  temp=128\n",
    "  z=0\n",
    "  for j in range(0,X.shape[0],128):\n",
    "    z+=1\n",
    "    X_1=first_X[j:temp,:]\n",
    "    prediction=np.matmul(X_1,W)\n",
    "    loss=prediction-first_y[j:temp,:] #####\n",
    "    gradient=np.matmul(X_1.T,loss)\n",
    "    W=W-lr*(1/X_1.shape[0])*gradient\n",
    "\n",
    "    temp=temp+ 128 if X.shape[0]-temp>=128 else X.shape[0]\n",
    "  if i % 1000 == 0:\n",
    "      print(z)\n",
    "      loss = np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "      print(\"Loss at iter {}: {}\".format(i, loss))\n",
    "loss=np.mean(np.square(np.matmul(X,W)-y_train))\n",
    "print(\"Loss: \", loss)\n",
    "print(\"W: \", W)\n",
    "saveW.append(W)\n",
    "saveloss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name_dict = dict(sorted(name_dict, key = lambda x : x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-415.93979903],\n",
       "       [   1.2317513 ],\n",
       "       [ 181.14588101]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(saveW,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24383.10171982266, 24411.114319291002, 24383.10171940595]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-415.47940439],\n",
       "       [   1.63051693],\n",
       "       [ 181.33377584]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveW[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Câu 3: Cho biết Weights tốt nhất cho các mô hình trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.concatenate([np.ones([X_test.shape[0],1]),X_test],axis=1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(1,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGD\n",
      "24383.10171982266\n",
      "[[-415.47940439]\n",
      " [   1.63051693]\n",
      " [ 181.33377584]]\n",
      "SGD\n",
      "24411.114319291002\n",
      "[[-415.93979903]\n",
      " [   1.2317513 ]\n",
      " [ 181.14588101]]\n",
      "Mini - GD\n",
      "24383.10171940595\n",
      "[[-415.48120459]\n",
      " [   1.63051922]\n",
      " [ 181.33412414]]\n"
     ]
    }
   ],
   "source": [
    "y_hat=[]\n",
    "y_pred = np.dot(X_test,saveW[0])\n",
    "y_hat.append(y_pred)\n",
    "print('BGD')\n",
    "print(saveloss[0])\n",
    "print(saveW[0])\n",
    "\n",
    "print('SGD')\n",
    "y_pred = np.dot(X_test,saveW[1])\n",
    "y_hat.append(y_pred)\n",
    "print(saveloss[1])\n",
    "print(saveW[1])\n",
    "\n",
    "print('Mini - GD')\n",
    "y_pred = np.dot(X_test,saveW[2])\n",
    "y_hat.append(y_pred)\n",
    "print(saveloss[2])\n",
    "print(saveW[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss train set</th>\n",
       "      <th>loss test set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BGD</th>\n",
       "      <td>24383.101720</td>\n",
       "      <td>19231.507487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>24411.114319</td>\n",
       "      <td>19292.747807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mini BGD</th>\n",
       "      <td>24383.101719</td>\n",
       "      <td>19231.487049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss train set  loss test set\n",
       "BGD         24383.101720   19231.507487\n",
       "SGD         24411.114319   19292.747807\n",
       "Mini BGD    24383.101719   19231.487049"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_table = {}\n",
    "loss_table['loss train set'] = [loss for loss in saveloss]\n",
    "loss_table['loss test set'] = [0.5*np.mean(np.square(y-y_test)) for y in y_hat]\n",
    "loss_table = pd.DataFrame(loss_table)\n",
    "loss_table.index = ['BGD','SGD','Mini BGD']\n",
    "loss_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy ta thấy Mini BGD cho ra kết quả tốt nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
